{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "# Exercise 2 - More Comparative Evaluation\n",
    "<br/>Student:\n",
    "<br/>se21m024\n",
    "<br/>Matriculation number: 1425616\n",
    "<br/>Thomas Stummer\n",
    "<br/><br/>The interpretation of the data can be found in the document <b><i>se21m024_Stummer_ml_ex2_comp_eval.pdf</i></b>.\n",
    "<br/><br/>\n",
    "The library <i>Surprise</i> (https://surprise.readthedocs.io/en/stable/index.html) was used to create the following results. The code is highly inspired by the example code provided by the libries official documentation.\n",
    "<br/><br/>\n",
    "Small data set: Heart Failure Prediction<br>\n",
    "The data set was provided by Davide Chicco, Giuseppe Jurman: Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making 20, 16 (2020) (https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5) and downloaded from https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data.\n",
    "<br/><br/>\n",
    "Big data set: Covertype<br>\n",
    "The data set was provided by Jock A. Blackard and Colorado State University and downloaded from https://archive.ics.uci.edu/ml/datasets/Covertype.\n",
    "<br/><br/>\n",
    "Music data set<br>\n",
    "Downloaded from Moodle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from librosa import display\n",
    "import librosa\n",
    "import numpy as np\n",
    "import datetime\n",
    "from collections import deque\n",
    "import progressbar\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import neighbors\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import scipy.stats.stats as st\n",
    "\n",
    "# Matriculation number: 1425616\n",
    "random_state = 1425616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set 1: Small Data Set: Heart Failure Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_set = pd.read_csv(\"..\\\\Exercise 5\\\\Data\\\\heartfailure\\\\heart_failure_clinical_records_dataset.csv\")\n",
    "heart_failure_data_set = pd.read_csv(\"C:\\Repositories\\_Tom\\FH\\Sem2\\DataScience\\Exercise 5\\Data\\heartfailure\\heart_failure_clinical_records_dataset.csv\")\n",
    "\n",
    "# Take only a subset for testing\n",
    "heart_failure_data_set = heart_failure_data_set[:100]\n",
    "\n",
    "# Split data in input features (X) and target (y) feature\n",
    "# The target feature is 'DEATH_EVENT' that indicates weither the person has died\n",
    "# Column 'time' is not used as feature due to the direct connection to the target feature 'death_event': https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data/discussion/178372\n",
    "heart_failure_data_set_X = heart_failure_data_set.loc[:,:'smoking']\n",
    "heart_failure_data_set_y = heart_failure_data_set.loc[:,'DEATH_EVENT':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set 2: Big Data Set: Covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covertype_data_set = pd.read_csv(\"./Data/covtype/covtype.data\", header=None)\n",
    "covertype_data_set = pd.read_csv(\"C:\\\\Repositories\\\\_Tom\\\\FH\\\\Sem2\\\\DataScience\\\\Exercise 5\\\\Data\\\\covtype\\\\covtype.data\", header=None)\n",
    "\n",
    "# Take only a subset for testing\n",
    "covertype_data_set = covertype_data_set[:100]\n",
    "\n",
    "# Split data in input features (X) and target (y) feature\n",
    "# The target feature is 'Forest cover type class' in column 54 than can be any value between 1 and 7 and indicates which type of vegetation is growing there mainly.\n",
    "covertype_data_set_X = covertype_data_set.loc[:,:53]\n",
    "covertype_data_set_y = covertype_data_set.loc[:,54:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set 3: Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 files\n",
      "\n",
      "Found the following classes: ['disco', 'metal']\n",
      "Transformed labels (first elements: [0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      "... done label encoding\n"
     ]
    }
   ],
   "source": [
    "# We need to construct our data set; unfortunately, we don't simply have a \"loadGTZanDataSet()\" function in SK-learn...\n",
    "# So we need to \n",
    "## Download our data set & extract it (one-time effort)\n",
    "## Run an audio feature extraction\n",
    "## Create the create the ground truth (label assignment, target, ...) \n",
    "\n",
    "# path to our audio folder\n",
    "# For the first run, download the images from http://kronos.ifs.tuwien.ac.at/GTZANmp3_22khz.zip, and unzip them to your folder\n",
    "#imagePath=\"../../ML_Data/GTZANmp3_22khz/\"\n",
    "imagePath=\"..\\\\..\\\\ML_Data\\\\GTZANmp3_22khz_sub\\\\\"\n",
    "\n",
    "# Find all songs in that folder; there are like 1.000 different ways to do this in Python, we chose this one :-)\n",
    "os.chdir(imagePath)\n",
    "fileNames = glob.glob(\"*/*.mp3\")\n",
    "numberOfFiles=len(fileNames)\n",
    "targetLabels=[]\n",
    "\n",
    "print( 'Found ' + str(numberOfFiles) + \" files\\n\")\n",
    "\n",
    "# The first step - create the ground truth (label assignment, target, ...) \n",
    "# For that, iterate over the files, and obtain the class label for each file\n",
    "# Basically, the class name is in the full path name, so we simply use that\n",
    "for fileName in fileNames:\n",
    "    pathSepIndex = fileName.index(\"\\\\\")\n",
    "    targetLabels.append(fileName[:pathSepIndex])\n",
    "\n",
    "# sk-learn can only handle labels in numeric format - we have them as strings though...\n",
    "# Thus we use the LabelEncoder, which does a mapping to Integer numbers\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(targetLabels) # this basically finds all unique class names, and assigns them to the numbers\n",
    "print( \"Found the following classes: \" + str(list(le.classes_)))\n",
    "\n",
    "# now we transform our labels to integers\n",
    "target = le.transform(targetLabels); \n",
    "music_target = target\n",
    "print( \"Transformed labels (first elements: \" + str(target[0:150]))\n",
    "\n",
    "# If we want to find again the label for an integer value, we can do something like this:\n",
    "# print list(le.inverse_transform([0, 18, 1]))\n",
    "\n",
    "print( \"... done label encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features using librosa (2022-05-22 12:26:39.104238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n",
      "  0% (0 of 22) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
      "  4% (1 of 22) |#                        | Elapsed Time: 0:00:03 ETA:   0:01:11\n",
      "  9% (2 of 22) |##                       | Elapsed Time: 0:00:06 ETA:   0:01:09\n",
      " 13% (3 of 22) |###                      | Elapsed Time: 0:00:10 ETA:   0:01:05\n",
      " 18% (4 of 22) |####                     | Elapsed Time: 0:00:13 ETA:   0:01:02\n",
      " 22% (5 of 22) |#####                    | Elapsed Time: 0:00:17 ETA:   0:00:58\n",
      " 27% (6 of 22) |######                   | Elapsed Time: 0:00:20 ETA:   0:00:54\n",
      " 31% (7 of 22) |#######                  | Elapsed Time: 0:00:24 ETA:   0:00:50\n",
      " 36% (8 of 22) |#########                | Elapsed Time: 0:00:27 ETA:   0:00:47\n",
      " 40% (9 of 22) |##########               | Elapsed Time: 0:00:30 ETA:   0:00:45\n",
      " 45% (10 of 22) |##########              | Elapsed Time: 0:00:34 ETA:   0:00:42\n",
      " 50% (11 of 22) |############            | Elapsed Time: 0:00:38 ETA:   0:00:39\n",
      " 54% (12 of 22) |#############           | Elapsed Time: 0:00:41 ETA:   0:00:35\n",
      " 59% (13 of 22) |##############          | Elapsed Time: 0:00:45 ETA:   0:00:31\n",
      " 63% (14 of 22) |###############         | Elapsed Time: 0:00:48 ETA:   0:00:28\n",
      " 68% (15 of 22) |################        | Elapsed Time: 0:00:52 ETA:   0:00:24\n",
      " 72% (16 of 22) |#################       | Elapsed Time: 0:00:55 ETA:   0:00:21\n",
      " 77% (17 of 22) |##################      | Elapsed Time: 0:00:59 ETA:   0:00:17\n",
      " 81% (18 of 22) |###################     | Elapsed Time: 0:01:02 ETA:   0:00:15\n",
      " 86% (19 of 22) |####################    | Elapsed Time: 0:01:06 ETA:   0:00:10\n",
      " 90% (20 of 22) |#####################   | Elapsed Time: 0:01:10 ETA:   0:00:07\n",
      " 95% (21 of 22) |######################  | Elapsed Time: 0:01:13 ETA:   0:00:03\n",
      "100% (22 of 22) |########################| Elapsed Time: 0:01:13 Time:  0:01:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... done (2022-05-22 12:27:56.156840)\n"
     ]
    }
   ],
   "source": [
    "# Now we do the actual feature extraction\n",
    "\n",
    "# This is a helper function that computes the differences between adjacent array values\n",
    "def differences(seq):\n",
    "    iterable = iter(seq)\n",
    "    prev = next(iterable)\n",
    "    for element in iterable:\n",
    "        yield element - prev\n",
    "        prev = element\n",
    "\n",
    "# This is a helper function that computes various statistical moments over a series of values, including mean, median, var, min, max, skewness and kurtosis (a total of 7 values)\n",
    "def statistics(numericList):\n",
    "    return [np.mean(numericList), np.median(numericList), np.var(numericList), np.float64(st.skew(numericList)), np.float64(st.kurtosis(numericList)), np.min(numericList), np.max(numericList)]\n",
    "\n",
    "print( \"Extracting features using librosa\" + \" (\" + str(datetime.datetime.now()) + \")\")\n",
    "\n",
    "# compute some features based on BPMs, MFCCs, Chroma\n",
    "data_bpm=[]\n",
    "data_bpm_statistics=[]\n",
    "data_mfcc=[]\n",
    "data_chroma=[]\n",
    "\n",
    "# This takes a bit, so let's show it with a progress bar\n",
    "with progressbar.ProgressBar(max_value=len(fileNames)) as bar:\n",
    "    for indexSample, fileName in enumerate(fileNames):\n",
    "        # Load the audio as a waveform `y`, store the sampling rate as `sr`\n",
    "        y, sr = librosa.load(fileName)\n",
    "\n",
    "        # run the default beat tracker\n",
    "        tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        # from this, we simply use the tempo as BPM feature\n",
    "        data_bpm.append([tempo])\n",
    "\n",
    "        # Then we compute a few statistics on the beat timings\n",
    "        beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "        # from the timings, compute the time differences between the beats\n",
    "        beat_intervals = np.array(deque(differences(beat_times)))\n",
    "\n",
    "        # And from this, take some statistics\n",
    "        # There might be a few files where the beat timings are not determined properly; we ignore them, resp. give them 0 values\n",
    "        if len(beat_intervals) < 1:\n",
    "            print( \"Errors with beat interval in file \" + fileName + \", index \" + str(indexSample) + \", using 0 values instead\")\n",
    "            data_bpm_statistics.append([tempo, 0, 0, 0, 0, 0, 0, 0])\n",
    "        else:\n",
    "            bpm_statisticsVector=[]\n",
    "            bpm_statisticsVector.append(tempo) # we also include the raw value of tempo\n",
    "            for stat in statistics(beat_intervals):  # in case the timings are ok, we actually compute the statistics\n",
    "                bpm_statisticsVector.append(stat) # and append it to the vector, which finally has 1 + 7 features\n",
    "            data_bpm_statistics.append(bpm_statisticsVector)\n",
    "\n",
    "        # Next feature are MFCCs; we take 12 coefficients; for each coefficient, we have around 40 values per second\n",
    "        mfccs=librosa.feature.mfcc(y=y, sr=sr, n_mfcc=12)\n",
    "        mfccVector=[]\n",
    "        for mfccCoefficient in mfccs: # we transform this time series by taking again statistics over the values\n",
    "            mfccVector.append(statistics(mfccCoefficient))\n",
    "\n",
    "        # Finally, this vector should have 12 * 7 features\n",
    "        data_mfcc.append(np.array(mfccVector).flatten())\n",
    "\n",
    "\n",
    "        # Last feature set - chroma (which is roughly similar to actual notes)\n",
    "        chroma=librosa.feature.chroma_stft(y=y, sr=sr);\n",
    "        chromaVector=[]\n",
    "        for chr in chroma: # similar to before, we get a number of time-series\n",
    "            chromaVector.append(statistics(chr)) # and we resolve that by taking statistics over the time series\n",
    "        # Finally, this vector should be be 12 * 7 features\n",
    "        data_chroma.append(np.array(chromaVector).flatten())\n",
    "\n",
    "        bar.update(indexSample)\n",
    "\n",
    "print( \".... done\" + \" (\" + str(datetime.datetime.now()) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN\n",
    "# kd tree was chosen to gain results within a reasonable amount of time\n",
    "def kNN (dataSetName, X_train, X_test, y_train, y_test, n_neighbors_values):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for n_neighbors in n_neighbors_values:\n",
    "            classifier = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm='kd_tree')\n",
    "\n",
    "            # Train classifier\n",
    "            start_time = datetime.datetime.now()\n",
    "            \n",
    "            #classifier.fit(X_train, y_train.values.ravel())\n",
    "            classifier.fit(X_train, y_train.ravel())\n",
    "\n",
    "            end_time = datetime.datetime.now()\n",
    "            training_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Predict test set on trained classifier\n",
    "            start_time = datetime.datetime.now()\n",
    "            y_test_predicted = classifier.predict(X_test)\n",
    "            end_time = datetime.datetime.now()\n",
    "            testing_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Compute metrics\n",
    "            acc = metrics.accuracy_score(y_test, y_test_predicted)\n",
    "            f1 = f1_score(y_true=y_test, y_pred=y_test_predicted, average='weighted')\n",
    "\n",
    "            # Store results\n",
    "            result = type('',(object,),{'n_neigbors': n_neighbors, 'training_time_sec': training_time_sec, 'testing_time_sec': testing_time_sec, 'acc': acc, 'f1': f1})()\n",
    "            results.append(result)\n",
    "\n",
    "    # Print results\n",
    "    print(dataSetName)\n",
    "    print('Algorithm | acc | f1 | training_time_sec | testing_time_sec')\n",
    "    for res in results:\n",
    "        print('k-NN (' + str(res.n_neigbors) + '-NN) | ' + str(round(res.acc, 3)) + ' | ' + str(round(res.f1, 3)) + ' | ' + str(res.training_time_sec) + ' sec | ' + str(res.testing_time_sec) + ' sec')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptron\n",
    "def perceptron(dataSetName, X_train, X_test, y_train, y_test, alpha_values):\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for alpha in alpha_values:\n",
    "                classifier = Perceptron(alpha=alpha, random_state=random_state)\n",
    "\n",
    "                # Train classifier\n",
    "                start_time = datetime.datetime.now()\n",
    "                classifier.fit(X_train, y_train.ravel())\n",
    "                end_time = datetime.datetime.now()\n",
    "                training_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "                # Predict test set on trained classifier\n",
    "                start_time = datetime.datetime.now()\n",
    "                y_test_predicted = classifier.predict(X_test)\n",
    "                end_time = datetime.datetime.now()\n",
    "                testing_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "                # Compute metrics\n",
    "                acc = metrics.accuracy_score(y_test, y_test_predicted)\n",
    "                f1 = f1_score(y_true=y_test, y_pred=y_test_predicted, average='weighted')\n",
    "\n",
    "                # Store results\n",
    "                result = type('',(object,),{'alpha': alpha, 'training_time_sec': training_time_sec, 'testing_time_sec': testing_time_sec, 'acc': acc, 'f1': f1})()\n",
    "                results.append(result)\n",
    "\n",
    "        # Print results\n",
    "        print(dataSetName)\n",
    "        print('Algorithm | acc | f1 | training_time_sec | testing_time_sec')\n",
    "        for res in results:\n",
    "                print('Perceptron (alpha: ' + str(res.alpha) + ') | ' + str(round(res.acc, 3)) + ' | ' + str(round(res.f1, 3)) + ' | ' + str(res.training_time_sec) + ' sec | ' + str(res.testing_time_sec) + ' sec')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree\n",
    "def decision_tree(dataSetName, X_train, X_test, y_train, y_test, max_features_values):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for max_features in max_features_values:\n",
    "            classifier = DecisionTreeClassifier(max_features=max_features, random_state=random_state) \n",
    "\n",
    "            # Train classifier\n",
    "            start_time = datetime.datetime.now()\n",
    "            classifier.fit(X_train, y_train.ravel())\n",
    "            end_time = datetime.datetime.now()\n",
    "            training_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Predict test set on trained classifier\n",
    "            start_time = datetime.datetime.now()\n",
    "            y_test_predicted = classifier.predict(X_test)\n",
    "            end_time = datetime.datetime.now()\n",
    "            testing_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Compute metrics\n",
    "            acc = metrics.accuracy_score(y_test, y_test_predicted)\n",
    "            f1 = f1_score(y_true=y_test, y_pred=y_test_predicted, average='weighted')\n",
    "\n",
    "            # Store results\n",
    "            result = type('',(object,),{'max_features': max_features, 'training_time_sec': training_time_sec, 'testing_time_sec': testing_time_sec, 'acc': acc, 'f1': f1})()\n",
    "            results.append(result)\n",
    "\n",
    "    # Print results\n",
    "    print(dataSetName)\n",
    "    print('Algorithm | acc | f1 | training_time_sec | testing_time_sec')\n",
    "    for res in results:\n",
    "        print('Decision tree (max features: ' + str(res.max_features) + ') | ' + str(round(res.acc, 3)) + ' | ' + str(round(res.f1, 3)) + ' | ' + str(res.training_time_sec) + ' sec | ' + str(res.testing_time_sec) + ' sec')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart_failure_prediction\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "Perceptron (alpha: 0.0001) | 0.303 | 0.141 | 0.003 sec | 0.001003 sec\n",
      "Perceptron (alpha: 0.001) | 0.303 | 0.141 | 0.003008 sec | 0.001013 sec\n",
      "Perceptron (alpha: 0.01) | 0.303 | 0.141 | 0.002001 sec | 0.000999 sec\n",
      "\n",
      "heart_failure_prediction\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "k-NN (1-NN) | 0.515 | 0.515 | 0.002037 sec | 0.002995 sec\n",
      "k-NN (2-NN) | 0.545 | 0.563 | 0.001989 sec | 0.002993 sec\n",
      "k-NN (3-NN) | 0.545 | 0.538 | 0.001019 sec | 0.002955 sec\n",
      "\n",
      "heart_failure_prediction\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "Decision tree (max features: None) | 0.576 | 0.576 | 0.003001 sec | 0.001 sec\n",
      "Decision tree (max features: sqrt) | 0.515 | 0.526 | 0.001 sec | 0.002001 sec\n",
      "Decision tree (max features: log2) | 0.515 | 0.526 | 0.001 sec | 0.001999 sec\n",
      "\n",
      "covertype\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "Perceptron (alpha: 0.0001) | 0.667 | 0.618 | 0.004001 sec | 0.000999 sec\n",
      "Perceptron (alpha: 0.001) | 0.667 | 0.618 | 0.003 sec | 0.002044 sec\n",
      "Perceptron (alpha: 0.01) | 0.667 | 0.618 | 0.003002 sec | 0.000994 sec\n",
      "\n",
      "covertype\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "k-NN (1-NN) | 0.788 | 0.774 | 0.002 sec | 0.003033 sec\n",
      "k-NN (2-NN) | 0.758 | 0.743 | 0.002036 sec | 0.002964 sec\n",
      "k-NN (3-NN) | 0.727 | 0.703 | 0.001046 sec | 0.002987 sec\n",
      "\n",
      "covertype\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "Decision tree (max features: None) | 0.818 | 0.812 | 0.002006 sec | 0.000996 sec\n",
      "Decision tree (max features: sqrt) | 0.636 | 0.625 | 0.002004 sec | 0.000995 sec\n",
      "Decision tree (max features: log2) | 0.636 | 0.61 | 0.002023 sec | 0.000981 sec\n",
      "\n",
      "music_bmp\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "Perceptron (alpha: 0.0001) | 0.25 | 0.1 | 0.001 sec | 0.0 sec\n",
      "Perceptron (alpha: 0.001) | 0.25 | 0.1 | 0.0 sec | 0.0 sec\n",
      "Perceptron (alpha: 0.01) | 0.25 | 0.1 | 0.001 sec | 0.0 sec\n",
      "\n",
      "music_bmp\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "k-NN (1-NN) | 0.625 | 0.645 | 0.0 sec | 0.000975 sec\n",
      "k-NN (2-NN) | 0.5 | 0.5 | 0.001 sec | 0.0 sec\n",
      "k-NN (3-NN) | 0.5 | 0.533 | 0.0 sec | 0.001001 sec\n",
      "\n",
      "music_bmp\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "Decision tree (max features: None) | 0.625 | 0.645 | 0.001037 sec | 0.000988 sec\n",
      "Decision tree (max features: sqrt) | 0.625 | 0.645 | 0.000999 sec | 0.0 sec\n",
      "Decision tree (max features: log2) | 0.625 | 0.645 | 0.001001 sec | 0.0 sec\n",
      "\n",
      "music_bpm_statistics\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "Perceptron (alpha: 0.0001) | 0.375 | 0.325 | 0.0 sec | 0.0 sec\n",
      "Perceptron (alpha: 0.001) | 0.375 | 0.325 | 0.0 sec | 0.0 sec\n",
      "Perceptron (alpha: 0.01) | 0.375 | 0.325 | 0.0 sec | 0.001 sec\n",
      "\n",
      "music_bpm_statistics\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "k-NN (1-NN) | 0.625 | 0.645 | 0.001034 sec | 0.001003 sec\n",
      "k-NN (2-NN) | 0.625 | 0.577 | 0.0 sec | 0.001034 sec\n",
      "k-NN (3-NN) | 0.5 | 0.533 | 0.0 sec | 0.000998 sec\n",
      "\n",
      "music_bpm_statistics\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "Decision tree (max features: None) | 0.625 | 0.643 | 0.001 sec | 0.0 sec\n",
      "Decision tree (max features: sqrt) | 0.375 | 0.409 | 0.001002 sec | 0.0 sec\n",
      "Decision tree (max features: log2) | 0.5 | 0.5 | 0.001033 sec | 0.0 sec\n",
      "\n",
      "music_chroma\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "Perceptron (alpha: 0.0001) | 0.125 | 0.056 | 0.001004 sec | 0.000997 sec\n",
      "Perceptron (alpha: 0.001) | 0.125 | 0.056 | 0.001001 sec | 0.0 sec\n",
      "Perceptron (alpha: 0.01) | 0.125 | 0.056 | 0.0 sec | 0.0 sec\n",
      "\n",
      "music_chroma\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "k-NN (1-NN) | 0.25 | 0.25 | 0.0 sec | 0.001018 sec\n",
      "k-NN (2-NN) | 0.375 | 0.405 | 0.0 sec | 0.001002 sec\n",
      "k-NN (3-NN) | 0.25 | 0.1 | 0.0 sec | 0.001003 sec\n",
      "\n",
      "music_chroma\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "Decision tree (max features: None) | 0.125 | 0.056 | 0.001035 sec | 0.0 sec\n",
      "Decision tree (max features: sqrt) | 0.625 | 0.643 | 0.001002 sec | 0.0 sec\n",
      "Decision tree (max features: log2) | 0.5 | 0.5 | 0.000993 sec | 0.0 sec\n",
      "\n",
      "music_mfcc\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "Perceptron (alpha: 0.0001) | 0.75 | 0.767 | 0.001003 sec | 0.0 sec\n",
      "Perceptron (alpha: 0.001) | 0.75 | 0.767 | 0.001001 sec | 0.0 sec\n",
      "Perceptron (alpha: 0.01) | 0.75 | 0.767 | 0.001001 sec | 0.0 sec\n",
      "\n",
      "music_mfcc\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "k-NN (1-NN) | 0.75 | 0.75 | 0.0 sec | 0.001005 sec\n",
      "k-NN (2-NN) | 0.75 | 0.75 | 0.0 sec | 0.000994 sec\n",
      "k-NN (3-NN) | 0.75 | 0.75 | 0.0 sec | 0.000967 sec\n",
      "\n",
      "music_mfcc\n",
      "Algorithm | acc | f1 | training_time_sec | testing_time_sec\n",
      "Decision tree (max features: None) | 0.625 | 0.577 | 0.001042 sec | 0.0 sec\n",
      "Decision tree (max features: sqrt) | 0.625 | 0.577 | 0.000961 sec | 0.0 sec\n",
      "Decision tree (max features: log2) | 0.625 | 0.577 | 0.0 sec | 0.0 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finally, we do classification\n",
    "\n",
    "# These are our feature sets; we will use each of them individually to train classifiers\n",
    "trainingSets = [\n",
    "                 ('heart_failure_prediction', heart_failure_data_set_X, heart_failure_data_set_y), \n",
    "                 ('covertype', covertype_data_set_X, covertype_data_set_y), \n",
    "                 ('music_bmp', data_bpm, music_target),\n",
    "                 ('music_bpm_statistics', data_bpm_statistics, music_target),\n",
    "                 ('music_chroma', data_chroma, music_target),\n",
    "                 ('music_mfcc', data_mfcc, music_target)\n",
    "               ]\n",
    "\n",
    "# set up a number of classifiers\n",
    "#classifiers = [neighbors.KNeighborsClassifier(),\n",
    " #              naive_bayes.GaussianNB(),\n",
    "  #             tree.DecisionTreeClassifier(),\n",
    "   #            ensemble.RandomForestClassifier(),\n",
    "    #           svm.SVC(),\n",
    "     #          svm.LinearSVC(),\n",
    "      #        ]\n",
    "\n",
    "def perceptron_with_parameters(data_set_name, X_train, X_test, y_train, y_test):\n",
    "    alpha_values = [0.0001, 0.001, 0.01]\n",
    "    perceptron(data_set_name, X_train, X_test, y_train, y_test, alpha_values)\n",
    "\n",
    "def kNN_with_parameters(data_set_name, X_train, X_test, y_train, y_test):\n",
    "    n_neighbors_values = [1, 2, 3]\n",
    "    kNN(data_set_name, X_train, X_test, y_train, y_test, n_neighbors_values)\n",
    "\n",
    "def decision_tree_with_parameters(dataSetName, X_train, X_test, y_train, y_test):\n",
    "    max_features_values = [None, 'sqrt', 'log2'] \n",
    "    decision_tree(dataSetName, X_train, X_test, y_train, y_test, max_features_values)\n",
    "\n",
    "classifiers = [\n",
    "                perceptron_with_parameters,\n",
    "                kNN_with_parameters,\n",
    "                decision_tree_with_parameters\n",
    "              ]\n",
    "\n",
    "for indexDataset, train_data_set in enumerate(trainingSets):\n",
    "\n",
    "    data_set_X = train_data_set[1]\n",
    "    data_set_y = train_data_set[2]\n",
    "\n",
    "    X, y = shuffle(data_set_X, data_set_y, random_state=random_state)\n",
    "\n",
    "    # Prepare a train/test set split: split 2/3 1/3 into training & test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=random_state)\n",
    "\n",
    "    if(train_data_set[0] == 'heart_failure_prediction' or train_data_set[0] == 'covertype'):\n",
    "        y_train = y_train.values\n",
    "\n",
    "    for indexClassifier, classifier in enumerate(classifiers):\n",
    "        # do the actual classification\n",
    "        classifier(train_data_set[0], X_train, X_test, y_train, y_test)   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

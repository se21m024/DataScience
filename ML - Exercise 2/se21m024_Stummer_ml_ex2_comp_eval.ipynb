{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "# Exercise 2 - More Comparative Evaluation\n",
    "<br/>Student:\n",
    "<br/>se21m024\n",
    "<br/>Matriculation number: 1425616\n",
    "<br/>Thomas Stummer\n",
    "<br/><br/>The interpretation of the data can be found in the document <b><i>se21m024_Stummer_ml_ex2_comp_eval.pdf</i></b>.\n",
    "<br/><br/>\n",
    "The library <i>Surprise</i> (https://surprise.readthedocs.io/en/stable/index.html) was used to create the following results. The code is highly inspired by the example code provided by the libries official documentation.\n",
    "<br/><br/>\n",
    "Small data set: Heart Failure Prediction<br>\n",
    "The data set was provided by Davide Chicco, Giuseppe Jurman: Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making 20, 16 (2020) (https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5) and downloaded from https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data.\n",
    "<br/><br/>\n",
    "Big data set: Covertype<br>\n",
    "The data set was provided by Jock A. Blackard and Colorado State University and downloaded from https://archive.ics.uci.edu/ml/datasets/Covertype.\n",
    "<br/><br/>\n",
    "Music data set<br>\n",
    "Downloaded from Moodle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from librosa import display\n",
    "import librosa\n",
    "import numpy as np\n",
    "import datetime\n",
    "from collections import deque\n",
    "import progressbar\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import neighbors\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import scipy.stats.stats as st\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Matriculation number: 1425616\n",
    "random_state = 1425616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set 1: Small Data Set: Heart Failure Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_set = pd.read_csv(\"..\\\\Exercise 5\\\\Data\\\\heartfailure\\\\heart_failure_clinical_records_dataset.csv\")\n",
    "heart_failure_data_set = pd.read_csv(\"C:\\Repositories\\_Tom\\FH\\Sem2\\DataScience\\Exercise 5\\Data\\heartfailure\\heart_failure_clinical_records_dataset.csv\")\n",
    "\n",
    "# Split data in input features (X) and target (y) feature\n",
    "# The target feature is 'DEATH_EVENT' that indicates weither the person has died\n",
    "# Column 'time' is not used as feature due to the direct connection to the target feature 'death_event': https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data/discussion/178372\n",
    "heart_failure_data_set_X = heart_failure_data_set.loc[:,:'smoking']\n",
    "heart_failure_data_set_y = heart_failure_data_set.loc[:,'DEATH_EVENT':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set 2: Big Data Set: Covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covertype_data_set = pd.read_csv(\"./Data/covtype/covtype.data\", header=None)\n",
    "covertype_data_set = pd.read_csv(\"C:\\\\Repositories\\\\_Tom\\\\FH\\\\Sem2\\\\DataScience\\\\Exercise 5\\\\Data\\\\covtype\\\\covtype.data\", header=None)\n",
    "\n",
    "# Take only a subset of 10 000 data points to reduce calculation time (the whole data set took far to long on my notebook)\n",
    "covertype_data_set = shuffle(covertype_data_set, random_state=random_state)\n",
    "covertype_data_set = covertype_data_set[:10000]\n",
    "\n",
    "# Split data in input features (X) and target (y) feature\n",
    "# The target feature is 'Forest cover type class' in column 54 than can be any value between 1 and 7 and indicates which type of vegetation is growing there mainly.\n",
    "covertype_data_set_X = covertype_data_set.loc[:,:53]\n",
    "covertype_data_set_y = covertype_data_set.loc[:,54:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set 3: Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to construct our data set; unfortunately, we don't simply have a \"loadGTZanDataSet()\" function in SK-learn...\n",
    "# So we need to \n",
    "## Download our data set & extract it (one-time effort)\n",
    "## Run an audio feature extraction\n",
    "## Create the create the ground truth (label assignment, target, ...) \n",
    "\n",
    "# path to our audio folder\n",
    "# For the first run, download the images from http://kronos.ifs.tuwien.ac.at/GTZANmp3_22khz.zip, and unzip them to your folder\n",
    "imagePath=\"../../ML_Data/GTZANmp3_22khz/\"\n",
    "\n",
    "# Find all songs in that folder; there are like 1.000 different ways to do this in Python, we chose this one :-)\n",
    "os.chdir(imagePath)\n",
    "fileNames = glob.glob(\"*/*.mp3\")\n",
    "numberOfFiles=len(fileNames)\n",
    "targetLabels=[]\n",
    "\n",
    "print( 'Found ' + str(numberOfFiles) + \" files\\n\")\n",
    "\n",
    "# The first step - create the ground truth (label assignment, target, ...) \n",
    "# For that, iterate over the files, and obtain the class label for each file\n",
    "# Basically, the class name is in the full path name, so we simply use that\n",
    "for fileName in fileNames:\n",
    "    pathSepIndex = fileName.index(\"\\\\\")\n",
    "    targetLabels.append(fileName[:pathSepIndex])\n",
    "\n",
    "# sk-learn can only handle labels in numeric format - we have them as strings though...\n",
    "# Thus we use the LabelEncoder, which does a mapping to Integer numbers\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(targetLabels) # this basically finds all unique class names, and assigns them to the numbers\n",
    "print( \"Found the following classes: \" + str(list(le.classes_)))\n",
    "\n",
    "# now we transform our labels to integers\n",
    "target = le.transform(targetLabels); \n",
    "music_target = target\n",
    "print( \"Transformed labels (first elements: \" + str(target[0:150]))\n",
    "\n",
    "# If we want to find again the label for an integer value, we can do something like this:\n",
    "# print list(le.inverse_transform([0, 18, 1]))\n",
    "\n",
    "print( \"... done label encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do the actual feature extraction\n",
    "\n",
    "# This is a helper function that computes the differences between adjacent array values\n",
    "def differences(seq):\n",
    "    iterable = iter(seq)\n",
    "    prev = next(iterable)\n",
    "    for element in iterable:\n",
    "        yield element - prev\n",
    "        prev = element\n",
    "\n",
    "# This is a helper function that computes various statistical moments over a series of values, including mean, median, var, min, max, skewness and kurtosis (a total of 7 values)\n",
    "def statistics(numericList):\n",
    "    return [np.mean(numericList), np.median(numericList), np.var(numericList), np.float64(st.skew(numericList)), np.float64(st.kurtosis(numericList)), np.min(numericList), np.max(numericList)]\n",
    "\n",
    "print( \"Extracting features using librosa\" + \" (\" + str(datetime.datetime.now()) + \")\")\n",
    "\n",
    "# compute some features based on BPMs, MFCCs, Chroma\n",
    "data_bpm=[]\n",
    "data_bpm_statistics=[]\n",
    "data_mfcc=[]\n",
    "data_chroma=[]\n",
    "\n",
    "# This takes a bit, so let's show it with a progress bar\n",
    "with progressbar.ProgressBar(max_value=len(fileNames), ) as bar:\n",
    "    for indexSample, fileName in enumerate(fileNames):\n",
    "        # Load the audio as a waveform `y`, store the sampling rate as `sr`\n",
    "        y, sr = librosa.load(fileName)\n",
    "\n",
    "        # run the default beat tracker\n",
    "        tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        # from this, we simply use the tempo as BPM feature\n",
    "        data_bpm.append([tempo])\n",
    "\n",
    "        # Then we compute a few statistics on the beat timings\n",
    "        beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "        # from the timings, compute the time differences between the beats\n",
    "        beat_intervals = np.array(deque(differences(beat_times)))\n",
    "\n",
    "        # And from this, take some statistics\n",
    "        # There might be a few files where the beat timings are not determined properly; we ignore them, resp. give them 0 values\n",
    "        if len(beat_intervals) < 1:\n",
    "            print( \"Errors with beat interval in file \" + fileName + \", index \" + str(indexSample) + \", using 0 values instead\")\n",
    "            data_bpm_statistics.append([tempo, 0, 0, 0, 0, 0, 0, 0])\n",
    "        else:\n",
    "            bpm_statisticsVector=[]\n",
    "            bpm_statisticsVector.append(tempo) # we also include the raw value of tempo\n",
    "            for stat in statistics(beat_intervals):  # in case the timings are ok, we actually compute the statistics\n",
    "                bpm_statisticsVector.append(stat) # and append it to the vector, which finally has 1 + 7 features\n",
    "            data_bpm_statistics.append(bpm_statisticsVector)\n",
    "\n",
    "        # Next feature are MFCCs; we take 12 coefficients; for each coefficient, we have around 40 values per second\n",
    "        mfccs=librosa.feature.mfcc(y=y, sr=sr, n_mfcc=12)\n",
    "        mfccVector=[]\n",
    "        for mfccCoefficient in mfccs: # we transform this time series by taking again statistics over the values\n",
    "            mfccVector.append(statistics(mfccCoefficient))\n",
    "\n",
    "        # Finally, this vector should have 12 * 7 features\n",
    "        data_mfcc.append(np.array(mfccVector).flatten())\n",
    "\n",
    "\n",
    "        # Last feature set - chroma (which is roughly similar to actual notes)\n",
    "        chroma=librosa.feature.chroma_stft(y=y, sr=sr);\n",
    "        chromaVector=[]\n",
    "        for chr in chroma: # similar to before, we get a number of time-series\n",
    "            chromaVector.append(statistics(chr)) # and we resolve that by taking statistics over the time series\n",
    "        # Finally, this vector should be be 12 * 7 features\n",
    "        data_chroma.append(np.array(chromaVector).flatten())\n",
    "\n",
    "        bar.update(indexSample)\n",
    "\n",
    "print( \".... done\" + \" (\" + str(datetime.datetime.now()) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN\n",
    "# kd tree was chosen to gain results within a reasonable amount of time\n",
    "def kNN (dataSetName, X_train, X_test, y_train, y_test, n_neighbors_values):\n",
    "\n",
    "    results = []\n",
    "    bestResult = None\n",
    "\n",
    "    for n_neighbors in n_neighbors_values:\n",
    "            classifier = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm='kd_tree')\n",
    "\n",
    "            # Train classifier\n",
    "            start_time = datetime.datetime.now()\n",
    "            \n",
    "            #classifier.fit(X_train, y_train.values.ravel())\n",
    "            classifier.fit(X_train, y_train.ravel())\n",
    "\n",
    "            end_time = datetime.datetime.now()\n",
    "            training_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Predict test set on trained classifier\n",
    "            start_time = datetime.datetime.now()\n",
    "            y_test_predicted = classifier.predict(X_test)\n",
    "            end_time = datetime.datetime.now()\n",
    "            testing_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Compute metrics\n",
    "            acc = metrics.accuracy_score(y_test, y_test_predicted)\n",
    "            f1 = f1_score(y_true=y_test, y_pred=y_test_predicted, average='weighted')\n",
    "\n",
    "            # Store results\n",
    "            result = type('',(object,),{'algorithm': 'k-NN', 'n_neigbors': n_neighbors, 'training_time_sec': training_time_sec, 'testing_time_sec': testing_time_sec, 'acc': acc, 'f1': f1, 'y_test_predicted': y_test_predicted})()\n",
    "            results.append(result)\n",
    "\n",
    "            # Cache best result for confusion matrix return value\n",
    "            if(bestResult is None or bestResult.f1 < result.f1):\n",
    "                bestResult = result\n",
    "\n",
    "    # Print results\n",
    "    print(dataSetName)\n",
    "    print('Algorithm | acc | f1 | training_time_sec | testing_time_sec')\n",
    "    for res in results:\n",
    "        print('k-NN (' + str(res.n_neigbors) + '-NN) | ' + str(round(res.acc, 3)) + ' | ' + str(round(res.f1, 3)) + ' | ' + str(res.training_time_sec) + ' sec | ' + str(res.testing_time_sec) + ' sec')\n",
    "    print()\n",
    "\n",
    "    return bestResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptron\n",
    "def perceptron(dataSetName, X_train, X_test, y_train, y_test, alpha_values):\n",
    "\n",
    "        results = []\n",
    "        bestResult = None\n",
    "\n",
    "        for alpha in alpha_values:\n",
    "                classifier = Perceptron(alpha=alpha, random_state=random_state)\n",
    "\n",
    "                # Train classifier\n",
    "                start_time = datetime.datetime.now()\n",
    "                classifier.fit(X_train, y_train.ravel())\n",
    "                end_time = datetime.datetime.now()\n",
    "                training_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "                # Predict test set on trained classifier\n",
    "                start_time = datetime.datetime.now()\n",
    "                y_test_predicted = classifier.predict(X_test)\n",
    "                end_time = datetime.datetime.now()\n",
    "                testing_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "                # Compute metrics\n",
    "                acc = metrics.accuracy_score(y_test, y_test_predicted)\n",
    "                f1 = f1_score(y_true=y_test, y_pred=y_test_predicted, average='weighted')\n",
    "\n",
    "                # Store results\n",
    "                result = type('',(object,),{'algorithm': 'perceptron', 'alpha': alpha, 'training_time_sec': training_time_sec, 'testing_time_sec': testing_time_sec, 'acc': acc, 'f1': f1, 'y_test_predicted': y_test_predicted})()\n",
    "                results.append(result)\n",
    "\n",
    "                # Cache best result for confusion matrix return value\n",
    "                if(bestResult is None or bestResult.f1 < result.f1):\n",
    "                        bestResult = result\n",
    "\n",
    "        # Print results\n",
    "        print(dataSetName)\n",
    "        print('Algorithm | acc | f1 | training_time_sec | testing_time_sec')\n",
    "        for res in results:\n",
    "                print('Perceptron (alpha: ' + str(res.alpha) + ') | ' + str(round(res.acc, 3)) + ' | ' + str(round(res.f1, 3)) + ' | ' + str(res.training_time_sec) + ' sec | ' + str(res.testing_time_sec) + ' sec')\n",
    "        print()\n",
    "        \n",
    "        return bestResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree\n",
    "def decision_tree(dataSetName, X_train, X_test, y_train, y_test, max_features_values):\n",
    "\n",
    "    results = []\n",
    "    bestResult = None\n",
    "\n",
    "    for max_features in max_features_values:\n",
    "            classifier = DecisionTreeClassifier(max_features=max_features, random_state=random_state) \n",
    "\n",
    "            # Train classifier\n",
    "            start_time = datetime.datetime.now()\n",
    "            classifier.fit(X_train, y_train.ravel())\n",
    "            end_time = datetime.datetime.now()\n",
    "            training_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Predict test set on trained classifier\n",
    "            start_time = datetime.datetime.now()\n",
    "            y_test_predicted = classifier.predict(X_test)\n",
    "            end_time = datetime.datetime.now()\n",
    "            testing_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Compute metrics\n",
    "            acc = metrics.accuracy_score(y_test, y_test_predicted)\n",
    "            f1 = f1_score(y_true=y_test, y_pred=y_test_predicted, average='weighted')\n",
    "\n",
    "            # Store results\n",
    "            result = type('',(object,),{'algorithm': 'decision_tree', 'max_features': max_features, 'training_time_sec': training_time_sec, 'testing_time_sec': testing_time_sec, 'acc': acc, 'f1': f1, 'y_test_predicted': y_test_predicted})()\n",
    "            results.append(result)\n",
    "\n",
    "            # Cache best result for confusion matrix return value\n",
    "            if(bestResult is None or bestResult.f1 < result.f1):\n",
    "                bestResult = result\n",
    "\n",
    "    # Print results\n",
    "    print(dataSetName)\n",
    "    print('Algorithm | acc | f1 | training_time_sec | testing_time_sec')\n",
    "    for res in results:\n",
    "        print('Decision Tree (max features: ' + str(res.max_features) + ') | ' + str(round(res.acc, 3)) + ' | ' + str(round(res.f1, 3)) + ' | ' + str(res.training_time_sec) + ' sec | ' + str(res.testing_time_sec) + ' sec')\n",
    "    print()\n",
    "\n",
    "    return bestResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "def svm_svc(dataSetName, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    results = []\n",
    "    bestResult = None\n",
    "    classifier = make_pipeline(StandardScaler(), SVC(random_state=random_state))\n",
    "\n",
    "    # Train classifier\n",
    "    start_time = datetime.datetime.now()\n",
    "    classifier.fit(X_train, y_train.ravel())\n",
    "    end_time = datetime.datetime.now()\n",
    "    training_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "    # Predict test set on trained classifier\n",
    "    start_time = datetime.datetime.now()\n",
    "    y_test_predicted = classifier.predict(X_test)\n",
    "    end_time = datetime.datetime.now()\n",
    "    testing_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "    # Compute metrics\n",
    "    acc = metrics.accuracy_score(y_test, y_test_predicted)\n",
    "    f1 = f1_score(y_true=y_test, y_pred=y_test_predicted, average='weighted')\n",
    "\n",
    "    # Store results\n",
    "    result = type('',(object,),{'algorithm': 'svm', 'training_time_sec': training_time_sec, 'testing_time_sec': testing_time_sec, 'acc': acc, 'f1': f1, 'y_test_predicted': y_test_predicted})()\n",
    "    results.append(result)\n",
    "\n",
    "    # Cache best result for confusion matrix return value\n",
    "    if(bestResult is None or bestResult.f1 < result.f1):\n",
    "        bestResult = result\n",
    "\n",
    "    # Print results\n",
    "    print(dataSetName)\n",
    "    print('Algorithm | acc | f1 | training_time_sec | testing_time_sec')\n",
    "    for res in results:\n",
    "        print('SVM | ' + str(round(res.acc, 3)) + ' | ' + str(round(res.f1, 3)) + ' | ' + str(res.training_time_sec) + ' sec | ' + str(res.testing_time_sec) + ' sec')\n",
    "    print()\n",
    "\n",
    "    return bestResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forests\n",
    "def random_forests(dataSetName, X_train, X_test, y_train, y_test, numbers_of_trees, max_features_values):\n",
    "\n",
    "    results = []\n",
    "    bestResult = None\n",
    "\n",
    "    for number_of_trees in numbers_of_trees:\n",
    "        for max_features in max_features_values:\n",
    "\n",
    "            classifier = RandomForestClassifier(n_estimators=number_of_trees, max_features=max_features, random_state=random_state)\n",
    "\n",
    "            # Train classifier\n",
    "            start_time = datetime.datetime.now()\n",
    "            classifier.fit(X_train, y_train.ravel())\n",
    "            end_time = datetime.datetime.now()\n",
    "            training_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Predict test set on trained classifier\n",
    "            start_time = datetime.datetime.now()\n",
    "            y_test_predicted = classifier.predict(X_test)\n",
    "            end_time = datetime.datetime.now()\n",
    "            testing_time_sec = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Compute metrics\n",
    "            acc = metrics.accuracy_score(y_test, y_test_predicted)\n",
    "            f1 = f1_score(y_true=y_test, y_pred=y_test_predicted, average='weighted')\n",
    "\n",
    "            # Store results\n",
    "            result = type('',(object,),{'algorithm': 'random_forests', 'number_of_trees': number_of_trees, 'max_features': max_features, 'training_time_sec': training_time_sec, 'testing_time_sec': testing_time_sec, 'acc': acc, 'f1': f1, 'y_test_predicted': y_test_predicted})()\n",
    "            results.append(result)\n",
    "\n",
    "            # Cache best result for confusion matrix return value\n",
    "            if(bestResult is None or bestResult.f1 < result.f1):\n",
    "                bestResult = result\n",
    "\n",
    "    # Print results\n",
    "    print(dataSetName)\n",
    "    print('Algorithm | acc | f1 | training_time_sec | testing_time_sec')\n",
    "    for res in results:\n",
    "        print('Random Forests (num trees: ' + str(res.number_of_trees) + ', max features: ' + str(res.max_features) + ') | ' + str(round(res.acc, 3)) + ' | ' + str(round(res.f1, 3)) + ' | ' + str(res.training_time_sec) + ' sec | ' + str(res.testing_time_sec) + ' sec')\n",
    "    print()\n",
    "\n",
    "    return bestResult\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_with_parameters(data_set_name, X_train, X_test, y_train, y_test):\n",
    "    alpha_values = [0.0001, 0.001, 0.01]\n",
    "    return perceptron(data_set_name, X_train, X_test, y_train, y_test, alpha_values)\n",
    "\n",
    "def kNN_with_parameters(data_set_name, X_train, X_test, y_train, y_test):\n",
    "    n_neighbors_values = [1, 2, 3]\n",
    "    return kNN(data_set_name, X_train, X_test, y_train, y_test, n_neighbors_values)\n",
    "\n",
    "def decision_tree_with_parameters(dataSetName, X_train, X_test, y_train, y_test):\n",
    "    max_features_values = [None, 'sqrt', 'log2'] \n",
    "    return decision_tree(dataSetName, X_train, X_test, y_train, y_test, max_features_values)\n",
    "\n",
    "def random_forests_with_parameters(dataSetName, X_train, X_test, y_train, y_test):\n",
    "    numbers_of_trees = [10, 100]\n",
    "    max_features_values = ['sqrt', 'log2'] \n",
    "    return random_forests(dataSetName, X_train, X_test, y_train, y_test, numbers_of_trees, max_features_values)\n",
    "\n",
    "classifiers = [\n",
    "                perceptron_with_parameters,\n",
    "                kNN_with_parameters,\n",
    "                decision_tree_with_parameters,\n",
    "                svm_svc,\n",
    "                random_forests_with_parameters\n",
    "              ]\n",
    "\n",
    "trainingSets = [\n",
    "                 ('heart_failure_prediction', heart_failure_data_set_X, heart_failure_data_set_y), \n",
    "                 ('covertype', covertype_data_set_X, covertype_data_set_y), \n",
    "                 ('music_bmp', data_bpm, music_target),\n",
    "                 ('music_bpm_statistics', data_bpm_statistics, music_target),\n",
    "                 ('music_chroma', data_chroma, music_target),\n",
    "                 ('music_mfcc', data_mfcc, music_target)\n",
    "               ]\n",
    "\n",
    "bestResult = None\n",
    "\n",
    "for indexDataset, train_data_set in enumerate(trainingSets):\n",
    "\n",
    "    data_set_X = train_data_set[1]\n",
    "    data_set_y = train_data_set[2]\n",
    "\n",
    "    X, y = shuffle(data_set_X, data_set_y, random_state=random_state)\n",
    "\n",
    "    # Prepare a train/test set split: split 2/3 1/3 into training & test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=random_state)\n",
    "\n",
    "    if(train_data_set[0] == 'heart_failure_prediction' or train_data_set[0] == 'covertype'):\n",
    "        y_train = y_train.values\n",
    "\n",
    "    for indexClassifier, classifier in enumerate(classifiers):\n",
    "        # do the actual classification\n",
    "        result = classifier(train_data_set[0], X_train, X_test, y_train, y_test)\n",
    "\n",
    "        if(train_data_set[0] != 'heart_failure_prediction' and train_data_set[0] != 'covertype'):\n",
    "            # Cache best result for confusion matrix return value\n",
    "            if(bestResult is None or bestResult.f1 < result.f1):\n",
    "                bestResult = result\n",
    "                bestResult.y_test = y_test\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "y_test = le.inverse_transform(bestResult.y_test)\n",
    "y_test_predicted = le.inverse_transform(bestResult.y_test_predicted)\n",
    "\n",
    "conf_matrix = pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_test_predicted, labels=list(le.classes_)),\n",
    "    index = ['true:' + str(label) for label in list(le.classes_)],\n",
    "    columns = ['pred:' + str(label) for label in list(le.classes_)]\n",
    ")\n",
    "\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "# Exercise 4 - Deep Learning\n",
    "<br/>Student:\n",
    "<br/>se21m024\n",
    "<br/>Matriculation number: 1425616\n",
    "<br/>Thomas Stummer\n",
    "<br/><br/>The interpretation of the data can be found in the document <b><i>se21m024_Stummer_ml_ex4_deep.pdf</i></b>.\n",
    "<br/><br/>\n",
    "The source code was heavily inspired by https://github.com/tuwien-musicir/DeepLearning_Tutorial/ and the code snippets provided by https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "<br/><br/>\n",
    "Data Set: CIFAR-10<br>\n",
    "\"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. The classes are completely mutually exclusive.\" [Description taken from https://www.cs.toronto.edu/~kriz/cifar.html]<br>\n",
    "CIFAR-10 python version downloaded from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz<br>\n",
    "Reference: Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import np_utils\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# Matriculation number: 1425616\n",
    "random_state = 1425616\n",
    "np.random.seed(random_state) # we initialize a random seed here to make the experiments repeatable with same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Images from the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 5 files\n"
     ]
    }
   ],
   "source": [
    "# Unpickle files\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "path = 'cifar-10-batches-py/data_batches'\n",
    "files = glob.glob(os.path.join(path, '*_*'))\n",
    "print('Imported ' + str(len(files)) + ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 50000 images.\n"
     ]
    }
   ],
   "source": [
    "# Import labeled images\n",
    "image_filenames = []\n",
    "images = np.empty([0,3072], dtype=np.ubyte)\n",
    "image_labels = []\n",
    "\n",
    "for filename in files:\n",
    "    file_data = unpickle(filename)\n",
    "    image_filenames.extend(file_data[b'filenames'])\n",
    "    images = np.append(images, file_data[b'data'], axis=0)\n",
    "    image_labels.extend(file_data[b'labels'])\n",
    "    \n",
    "print('Imported ' + str(len(images)) + ' images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'roe_deer_s_000207.png'\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAHnUlEQVR4nDVW2Y5kyW3lFhF3yayqrpleRi2PRhsECAJsQPoC/T+gLzC0wZI93VXdlXnzxo0ILn4oiQ98InkOeAiS+LNff8fMiIBAxISk0zTpwDF0WfN2+9oOA08RyAJEcD6vROly2dzdzERkXZfWKoBPc3EPUxcRZupD3ZxyzkQEgA4OqNMsxMAC4bhdemh5rU5EIpJzjghELKW4OyJG+PX6UuuuNoiglHw6rcTo4UwIABIRRPTqT6fpdDfp0H1XliCUfT8ikBinaRJBs7HXnUgR+JUWoqckZVpYEMCJo+7b0dr5fBbmeWWJCBGJCOaEmPdbP47mSjnLvjdVA4yUmIl7P4ggSQKg1joAMNM0zWUiFiQKIhxDAePu7iTC7j6aiYiklAK81RpmDlrrbZlnpsRkREoUEdqtTnMpOQ3tPkgojnpQopKyj6bdck4gFIqCCQyHKhG7iwCAqg+tjM7orXUWkYTCdFqnJDy6OUSa0jQXHaO1bgf4iLAAhv267/XKzKUUER5jjDHMnJlTSZhISsn73ohgWhZrIzF3j6fruJvxbl2nsl6v+14Pa3AbrbWmqqBBSESkqmYKgRDoFod2CIhARIJAYeEEcv9wfnm55SwRmFN6c3e61N6vzR3d43Lbtm2PwH4YICIiBgM4vOrrDoCIAkBj2OuAITJARAAEZWbZtoub67DrVt+cpg9vzt+eDMbzPsaX5xdVNwMECHciQsAIj4iIAEAAeEUCAABCDAAgYo9w89u2905ytMpMAGQBtfeJ7d3jfd/rf386nCgLg5BHaG/MGBEQ4UQBCBGvYISCCP+mT+7xysYDR3O5f7gHqK2aQ0yFaVRp9G6lf2wMBnGYYSQpmdPQMcYwB0RiQCbLiYQJgXPivR2Ho3m4mjAhCQFGqLRDE+fIwW4/v79/FJzGeCvpXTn9da9wKloHmIdH650BT6k8Fnh7zsucl4kTAZrfv33/l6fbn/78fwOMKDwczJAoIKTeGoIQyv26zDk/Xb80jofl/Jv7M6B9in6OOCFLYbR0l6c36/mcQqDraGA9M1Pi9xOdP5z7y/M1pr9//lINAxzcRESsd4i+rCeg+N/b5qF4u/2qlI8z/ZZ4S8uEZUY2UgGcSNi1Oh4DurqAKM5G6cfPz+z79+V2LQ8/fvXqREwMjuCSEzJTyWF6XJS88JSWS7Nvip1KXhwhUE3DuwEebMysEeoDwJEkwoTZI172fmkR3M9LrpzMIQFEoDx+OzMzIERX9IjMyzQPxiewB5LYju04jHHKGRl7mPVh/SCCqcwwlFQJ21X1c/cnlQ+Y3szxYnooYIAEyZtvVgRordMky5TCgwgxoLs64Dylo1ekIFcI1LA+RteRlzlnYSLtfWu3G2CbExUpmb/Lpw3rdVjfK5BLAgdEEHSLMF1SyUCZeA6cgdHGPM+UZLS+132Yck4P79+Wh7NH4LDosshCABNLGTb18WY5xWndhyHGddzklx9/wszLum6Xy8vz08N61luNY8QYlz5gGBGtaRIRyrLenb//4YfH//hOE6npuB0zc5nTtu+G1NWu//yff/7tH2/n+7e//MnHj+89uvzx939Qs5yzq20vl6dPn/ayTTlnyWqmqub27tt3Dw/3jjCfVkkJCLsbMed7itE5weP5zMyE+HmdwmCaH374xc/LzAwuZxEnJiRKPCyw6/c//enHjx9FBEWaDiJc5yUiamtHb4YxszBA672URYmUQsfIFJPk9e78u//6T4o0zUWjJ5kp54wIKcte61/+9ndJ+f2H7+Zl1TAkZ4Jwba0+PX/ScUyJl5xyZmYsiRUGJfJjxHDrbgZgIYhTQUK9W9YpJRGRAJim6euXl1TK/cPD6XQaYwBC7z0iWmvKo+T8uukSs6oiAQO7e8oZIiIi52xmOeWImKYSETmJ6hBEZGYze3z8pqQ5pWRmESHMavr6c7zmE1GtVVUjYlmWWusYo/eeU+q9m9k0Tfu+R1hEqNrtdsuZSE1fj/48zymliAAADweAnHMpZZomVW2tAcC6rjnn17sIAGMMMxtjRIS7v4b9+01xVa21CTP33pnZzYiImVNK7tZ6J8Jaa85lnmdV3batlDKGuvsrwGldzR3gtUkREfM8HcdRa22tMQshkfXx+ccfW629t5QIKQAMMGyoKxDw5eul1jY0kIQ45TLNy+ym9XYDc7JInE/LKacSFlMpACEpkaRjKInQaO28ns7rCSEAAzG22xUxhKXkUvKkwy5fL2aGSIgEgO6mOrR3GwPcj6MSEQK01nprEA4QKWciQgjJZUo5q+oYw9z5XyYpwTRlZn58fPj0+TOGu6mbRoSpIzIzt1aZGZBvt2vOZduudXdAWJb1vK4Q0a43cTOI2LYNCVnkVToAYJYxWj30tt/u7taUZN93Rigly7RAxBGho7a2f/P2g7sTwem03K5fEKEdN2LC8N47DR37vpu7iKgqAJQy7fuu2gHj+fmp1n2epzAbrfWjtnpkKcJZRNzttl97r5LoaDXAchIhPPb96/Ozmy13Z1rXtdZbmAqzm26Xr63eEGBoJwJmSiJhNvpRkhAGUpiPbbscx66jL8scCNfry+XyfNuu7hbhRAChx7Exx/8DGpWSqK/H22EAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x23CDBF97C10>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a selected image to check\n",
    "i=12645\n",
    "print (image_filenames[i])\n",
    "image = images[i]\n",
    "Image.merge('RGB', (\n",
    "    Image.fromarray(np.asarray(np.split(image[0:1024], indices_or_sections=32))),\n",
    "    Image.fromarray(np.asarray(np.split(image[1024:2048], indices_or_sections=32))),\n",
    "    Image.fromarray(np.asarray(np.split(image[2048:3072], indices_or_sections=32)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Groundtruth based on labels:\n",
    "\n",
    "In this data set, all images are labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 25 labels:\n",
      "[6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3, 6, 6, 2]\n",
      "\n",
      "Unique labels:\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "\n",
      "Groundtruth Statistics:\n",
      "Label 0 : 5000\n",
      "Label 1 : 5000\n",
      "Label 2 : 5000\n",
      "Label 3 : 5000\n",
      "Label 4 : 5000\n",
      "Label 5 : 5000\n",
      "Label 6 : 5000\n",
      "Label 7 : 5000\n",
      "Label 8 : 5000\n",
      "Label 9 : 5000\n"
     ]
    }
   ],
   "source": [
    "# look at the first 25 classes\n",
    "print('First 25 labels:\\n' + str(image_labels[0:25]) + '\\n')\n",
    "\n",
    "unique_image_labels = set(image_labels)\n",
    "print('Unique labels:\\n' + str(unique_image_labels) + '\\n')\n",
    "\n",
    "print(\"Groundtruth Statistics:\")\n",
    "for v in unique_image_labels:\n",
    "    print(\"Label\", v, \":\", image_labels.count(v))\n",
    "\n",
    "image_classes = np_utils.to_categorical(image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "Here we use <b>Zero-mean Unit-variance standardization</b> which means we deduct the mean and divide by the standard deviation.\n",
    "\n",
    "(Note: Here, we do this \"flat\", i.e. one mean and std.dev. for the whole image is computed over all pixels (not per pixel); in RGB images, standardization can be done e.g. for each colour channel individually; in other/non-image data sets, attribute-wise standardization should be applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 255)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.min(), images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120.70756512369792, 64.1500758911213)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = images.mean()\n",
    "stddev = images.std()\n",
    "mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.5247951877342226e-17, 1.0000000000000022)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_std = (images - mean) / stddev\n",
    "images_std = np.array(images_std, dtype=float)\n",
    "images_std.mean(), images_std.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.8816433721538972, 2.09341038199596)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_std.min(), images_std.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating NN Models in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1) Fully Connected NN\n",
    "\n",
    "For a fully connected neural network, the x and y axis of an image do not play a role at all. All pixels are considered as a completely individual input to the neural network. Therefore the 2D image arrays have to be flattened to a vector. For our current data set this is already the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "#  flatten images to vectors\n",
    "#images_flat = images_std.reshape(images_std.shape[0],-1)\n",
    "#print(images_flat.shape)\n",
    "print(images_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape for NN: 3072\n"
     ]
    }
   ],
   "source": [
    "# find out input shape for NN, which is just a long vector (32 x 32 x 3 = 1024 x 3 = 3072)\n",
    "input_shape = images_std.shape[1]\n",
    "print('Input shape for NN: ' + str(input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Model\n",
    "\n",
    "In Keras, one can choose between a Sequential model and a Graph model. Sequential models are the standard case. Graph models are for parallel networks and use the functional API (see Music/Speech tutorial).\n",
    "\n",
    "Here we create a sequential model with 2 fully connected (a.k.a. 'dense') layers containing 256 units each.\n",
    "\n",
    "The output unit is a Single sigmoid unit which can predict values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_37 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 855,050\n",
      "Trainable params: 855,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# simple Fully-connected network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_dim=input_shape))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dense(len(unique_image_labels), activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss Function and Optimizer Strategy: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'categorical_crossentropy'  # 'categorical_crossentropy' for multi-class problems\n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compiling the model\n",
    "# This creates the whole model structure in memory. \n",
    "# If you use GPU computation, here GPU compatible structures and code is generated.\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.8237 - accuracy: 0.3621\n",
      "Epoch 2/7\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6947 - accuracy: 0.4129\n",
      "Epoch 3/7\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6427 - accuracy: 0.4345\n",
      "Epoch 4/7\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6044 - accuracy: 0.4475\n",
      "Epoch 5/7\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5727 - accuracy: 0.4603\n",
      "Epoch 6/7\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5442 - accuracy: 0.4708\n",
      "Epoch 7/7\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5159 - accuracy: 0.4819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23cac67c670>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "epochs = 7\n",
    "model.fit(images_std, image_classes, batch_size=32, epochs=epochs) #, validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 3s 2ms/step\n",
      "Accuracy on training set: 0.49134\n"
     ]
    }
   ],
   "source": [
    "# verify accuracy on train set\n",
    "predictions = model.predict(images_std)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "image_labels_for_comparison = np.array(image_labels)\n",
    "accuracy_on_training_set = accuracy_score(image_labels_for_comparison, predicted_classes)\n",
    "print('Accuracy on training set: ' + str(accuracy_on_training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the accuracy on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1983597910.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [176]\u001b[1;36m\u001b[0m\n\u001b[1;33m    print \"Found\", len(files), \"files\"\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "path = 'data/CarData/TestImages'\n",
    "files = glob.glob(os.path.join(path, '*.pgm'))\n",
    "print \"Found\", len(files), \"files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from image_preprocessing import resize_and_crop\n",
    "\n",
    "test_images = []\n",
    "\n",
    "for filename in files:\n",
    "    with Image.open(filename) as img:\n",
    "        img_resized = resize_and_crop(img,target_width=100,target_height=40)\n",
    "        test_images.append(np.array(img_resized))\n",
    "        #print img.size, img_resized.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=4\n",
    "Image.fromarray(test_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make 1 big array again from list\n",
    "test_images = np.array(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Test Set\n",
    "\n",
    "The test data has to be standardized <b>in the same way</b> as the training data for compatibility with the model! That means, we take the mean and standard deviation of the <i>training data</i> to transform also the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NO! we take the same mean and stddev from the training data above!\n",
    "#mean = test_images.mean()\n",
    "#stddev = test_images.std()\n",
    "#print mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_images.mean(), test_images.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_images = (test_images - mean) / stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Images for Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_images_flat = test_images.reshape(test_images.shape[0],-1)\n",
    "print test_images_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict_classes(test_images_flat)\n",
    "# show 30 first predictions\n",
    "test_pred[0:30,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Groundtruth:\n",
    "# this TEST SET contains ONLY CARS on images! \n",
    "# Thus all the test classes are 1\n",
    "test_classes = [1] * len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's count the number of ones ...\n",
    "test_pred.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As ALL our test classes are 1, counting the number of 1's and dividing by number of files gives us the Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pred.sum() / 170.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real way to do it is to compare the predictions (test_pred) with the ground truth (test_classes) and sum up the correct ones.\n",
    "This is exactly what the scikit-learn function <i>accuracy_score</i> does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(test_classes, test_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the Test Set is rather low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "A Convolutional Neural Network (ConvNet or CNN) is a type of (deep) Neural Network that is well-suited for 2D axes data, such as images, as it is optimized for learning from spatial proximity. Its core elements are 2D filter kernels which essentially learn the weights of the Neural Network, and downscaling functions such as Max Pooling.\n",
    "\n",
    "A CNN can have one or more Convolution layers, each of them having an arbitrary number of N filters (which define the depth of the CNN layer), following typically by a pooling step, which groups neighboring pixels together and thus reduces the image resolution by retaining only the maximum values of neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "Our input to the CNN is the standardized version of the original image array.\n",
    "\n",
    "#### Adding the channel\n",
    "\n",
    "For CNNs, we need to add a dimension for the color channel to the data. RGB images typically have an 3rd dimension with the color. \n",
    "<b>For greyscale images we need to add an extra dimension for compatibility with the CNN implementation.</b>\n",
    "\n",
    "In Theano, traditionally the color channel was the <b>first</b> dimension in the image shape. \n",
    "In Tensorflow, the color channel is the <b>last</b> dimension in the image shape. \n",
    "\n",
    "This can be configured now in ~/.keras/keras.json: \"image_dim_ordering\": \"th\" or \"tf\" with \"tf\" (Tensorflow) being the default image ordering even though you use Theano. Depending on this, use one of the code lines below.\n",
    "\n",
    "For greyscale images, we add the number 1 as the depth of the additional dimension of the input shape (for RGB color images, the number of channels is 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_channels = 1 # for grey-scale, 3 for RGB, but usually already present in the data\n",
    "\n",
    "if keras.backend.image_dim_ordering() == 'th':\n",
    "    # Theano ordering (~/.keras/keras.json: \"image_dim_ordering\": \"th\")\n",
    "    train_img = img_array.reshape(img_array.shape[0], n_channels, img_array.shape[1], img_array.shape[2])\n",
    "    test_img = test_images.reshape(test_images.shape[0], n_channels, test_images.shape[1], test_images.shape[2])\n",
    "else:\n",
    "    # Tensorflow ordering (~/.keras/keras.json: \"image_dim_ordering\": \"tf\")\n",
    "    train_img = img_array.reshape(img_array.shape[0], img_array.shape[1], img_array.shape[2], n_channels)\n",
    "    test_img = test_images.reshape(test_images.shape[0], test_images.shape[1], test_images.shape[2], n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of images)\n",
    "    \n",
    "input_shape = train_img.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN model\n",
    "\n",
    "You may try to change the following to see the impact on the result:\n",
    "* number of filters\n",
    "* filter kernel size (e.g. 3 x 3, 5 x 5, ...)\n",
    "* adding/not adding Batch Normalization\n",
    "* adding/not adding ReLU Activation\n",
    "* adding/not adding Max Pooling\n",
    "* changing Pooling size (e.g. 1 x 2, 2 x 2, 2 x 1, or more)\n",
    "* adding/changing/removing Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createMyModel():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    n_filters = 16\n",
    "    # this applies n_filters convolution filters of size 5x5 resp. 3x3 each in the 2 layers below\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(Convolution2D(n_filters, 3, 3, border_mode='valid', input_shape=input_shape))\n",
    "    # input shape: 100x100 images with 3 channels -> input_shape should be (3, 100, 100) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))  # ReLu activation\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))) # reducing image resolution by half\n",
    "    model.add(Dropout(0.3))  # random \"deletion\" of %-portion of units in each batch\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Convolution2D(n_filters, 3, 3))  # input_shape is only needed in 1st layer\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten()) # Note: Keras does automatic shape inference.\n",
    "    \n",
    "    # Full Layer\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = createMyModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you get OverflowError: Range exceeds valid bounds in the above box, check the correct Theano vs. Tensorflow ordering in the box before and your keras.json configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "loss = 'binary_crossentropy' \n",
    "optimizer = 'sgd' \n",
    "#optimizer = SGD(lr=0.001)  # possibility to adapt the learn rate\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAINING the model\n",
    "epochs = 15\n",
    "history = model.fit(train_img, classes, batch_size=32, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you get ValueError: GpuDnnConv images and kernel must have the same stack size, best upgrade Keras to 1.2.0 as we experienced this problem when using BatchNormalization in Keras 1.1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Again, our Accuracy rises quickly to almost 100%, with the CNN now even faster than with the Fully Connected Network.\n",
    "But is our model really good at predicting unseen data?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Validation Data\n",
    "\n",
    "We split off 10 % of the training data and use it as independend validation set to verify how good we are\n",
    "on an independent data (not used for training) in each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recreate and recompile the model (otherwise we continue learning)\n",
    "model = createMyModel()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# train with showing accuracy on split off validation data\n",
    "history = model.fit(train_img, classes, batch_size=32, nb_epoch=epochs, validation_split=0.1) # portion of val. data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on the split-off validation data are quite high (usually similar, but not as high as on the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Test Set as Validation Set\n",
    "\n",
    "<b>Note: This usually is not recommended as during experimentation you will overfit also to the test data.</b>\n",
    "\n",
    "We show it here only for demonstration purposes to see how (bad) the validation accuracy is on our independet test data. The recommended way is to have a separate training, validation and test set (i.e. 3 splits or separate data sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recreate and recompile the model (otherwise we continue learning)\n",
    "model = createMyModel()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# show result on Test Data while training \n",
    "# we use test data as validation data to see direct results (usually NOT RECOMMENDED due to overfitting to the problem!)\n",
    "validation_data = (test_img, test_classes)\n",
    "\n",
    "history = model.fit(train_img, classes, batch_size=32, nb_epoch=epochs, validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>On the test set we perform really bad: less than 10% Accuracy!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict_classes(test_img)\n",
    "# show 35 first predictions\n",
    "test_pred[0:35,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test_img)\n",
    "accuracy_score(test_classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Training Curve\n",
    "\n",
    "The `model.fit` function returns a history including the evolution of training and validation loss and accuracy. We can plot it to see a nice training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist = history.history\n",
    "hist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this will only work if you have matplotlib installed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "colors = {'loss':'r', 'acc':'b', 'val_loss':'m', 'val_acc':'g'}\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Training Curve\") \n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "for measure in hist.keys():\n",
    "    color = colors[measure]\n",
    "    plt.plot(range(1,epochs+1), hist[measure], color + '-', label=measure)  # use last 2 values to draw line\n",
    "\n",
    "plt.legend(loc='upper left', scatterpoints = 1, frameon=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: We are completely overfitting!\n",
    "While we achieve nearly 100% on the Training Set, the generalization to the Test Set is really bad, with an Accuracy of about only 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Augmentation\n",
    "\n",
    "Increase the training set by adding more images: Rotate, shift, flip and scale the original images to generate additional examples that will help the Neural Network to generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ImageDataGenerator needs the classes as Numpy array instead of normal list\n",
    "classes_array = np.array(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) # enforce repeatable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recreate and recompile the model (otherwise we continue learning)\n",
    "model = createMyModel()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "history = model.fit_generator(datagen.flow(train_img, classes_array, batch_size=16),\n",
    "                    samples_per_epoch=len(train_img), nb_epoch=epochs,\n",
    "                    validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set (with Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict_classes(test_img)\n",
    "test_pred[0:35,0] # show 35 first predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test_img)\n",
    "accuracy_score(test_classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Conclusion: With Data Augmentation, the model has more diverse training examples and generalizes much better. The Accuracy on the Test Set increases from less than 10% to more than 55% up to 75%!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Plotting the Training Curve with Data Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist = history.history\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Training Curve\") \n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "for measure in hist.keys():\n",
    "    color = colors[measure]\n",
    "    plt.plot(range(1,epochs+1), hist[measure], color + '-', label=measure)  # use last 2 values to draw line\n",
    "\n",
    "plt.legend(loc='upper left', scatterpoints = 1, frameon=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is not ideal, but we see both training (blue) and validation (green) Accuracy going up, the training loss going continuously down and the validation loss fluctuation, but not exploding like previously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

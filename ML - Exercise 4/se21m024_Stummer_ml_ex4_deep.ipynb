{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "# Exercise 4 - Deep Learning\n",
    "<br/>Student:\n",
    "<br/>se21m024\n",
    "<br/>Matriculation number: 1425616\n",
    "<br/>Thomas Stummer\n",
    "<br/><br/>The interpretation of the data can be found in the document <b><i>se21m024_Stummer_ml_ex4_deep.pdf</i></b>.\n",
    "<br/><br/>\n",
    "The source code was heavily inspired by https://github.com/tuwien-musicir/DeepLearning_Tutorial/ and the code snippets provided by https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "<br/><br/>\n",
    "Data Set: CIFAR-10<br>\n",
    "\"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. The classes are completely mutually exclusive.\" [Description taken from https://www.cs.toronto.edu/~kriz/cifar.html]<br>\n",
    "CIFAR-10 python version downloaded from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz<br>\n",
    "Reference: Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "%matplotlib inline \n",
    "\n",
    "# Matriculation number: 1425616\n",
    "random_state = 1425616\n",
    "np.random.seed(random_state) # we initialize a random seed here to make the experiments repeatable with same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Images from the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 5 files\n"
     ]
    }
   ],
   "source": [
    "# Unpickle files\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "path = 'cifar-10-batches-py/data_batches'\n",
    "files = glob.glob(os.path.join(path, '*_*'))\n",
    "print('Imported ' + str(len(files)) + ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 50000 images.\n"
     ]
    }
   ],
   "source": [
    "# Import labeled images\n",
    "image_filenames = []\n",
    "images = np.empty([0,3072], dtype=np.ubyte)\n",
    "image_labels = []\n",
    "\n",
    "for filename in files:\n",
    "    file_data = unpickle(filename)\n",
    "    image_filenames.extend(file_data[b'filenames'])\n",
    "    images = np.append(images, file_data[b'data'], axis=0)\n",
    "    image_labels.extend(file_data[b'labels'])\n",
    "    \n",
    "print('Imported ' + str(len(images)) + ' images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'roe_deer_s_000207.png'\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAHnUlEQVR4nDVW2Y5kyW3lFhF3yayqrpleRi2PRhsECAJsQPoC/T+gLzC0wZI93VXdlXnzxo0ILn4oiQ98InkOeAiS+LNff8fMiIBAxISk0zTpwDF0WfN2+9oOA08RyAJEcD6vROly2dzdzERkXZfWKoBPc3EPUxcRZupD3ZxyzkQEgA4OqNMsxMAC4bhdemh5rU5EIpJzjghELKW4OyJG+PX6UuuuNoiglHw6rcTo4UwIABIRRPTqT6fpdDfp0H1XliCUfT8ikBinaRJBs7HXnUgR+JUWoqckZVpYEMCJo+7b0dr5fBbmeWWJCBGJCOaEmPdbP47mSjnLvjdVA4yUmIl7P4ggSQKg1joAMNM0zWUiFiQKIhxDAePu7iTC7j6aiYiklAK81RpmDlrrbZlnpsRkREoUEdqtTnMpOQ3tPkgojnpQopKyj6bdck4gFIqCCQyHKhG7iwCAqg+tjM7orXUWkYTCdFqnJDy6OUSa0jQXHaO1bgf4iLAAhv267/XKzKUUER5jjDHMnJlTSZhISsn73ohgWhZrIzF3j6fruJvxbl2nsl6v+14Pa3AbrbWmqqBBSESkqmYKgRDoFod2CIhARIJAYeEEcv9wfnm55SwRmFN6c3e61N6vzR3d43Lbtm2PwH4YICIiBgM4vOrrDoCIAkBj2OuAITJARAAEZWbZtoub67DrVt+cpg9vzt+eDMbzPsaX5xdVNwMECHciQsAIj4iIAEAAeEUCAABCDAAgYo9w89u2905ytMpMAGQBtfeJ7d3jfd/rf386nCgLg5BHaG/MGBEQ4UQBCBGvYISCCP+mT+7xysYDR3O5f7gHqK2aQ0yFaVRp9G6lf2wMBnGYYSQpmdPQMcYwB0RiQCbLiYQJgXPivR2Ho3m4mjAhCQFGqLRDE+fIwW4/v79/FJzGeCvpXTn9da9wKloHmIdH650BT6k8Fnh7zsucl4kTAZrfv33/l6fbn/78fwOMKDwczJAoIKTeGoIQyv26zDk/Xb80jofl/Jv7M6B9in6OOCFLYbR0l6c36/mcQqDraGA9M1Pi9xOdP5z7y/M1pr9//lINAxzcRESsd4i+rCeg+N/b5qF4u/2qlI8z/ZZ4S8uEZUY2UgGcSNi1Oh4DurqAKM5G6cfPz+z79+V2LQ8/fvXqREwMjuCSEzJTyWF6XJS88JSWS7Nvip1KXhwhUE3DuwEebMysEeoDwJEkwoTZI172fmkR3M9LrpzMIQFEoDx+OzMzIERX9IjMyzQPxiewB5LYju04jHHKGRl7mPVh/SCCqcwwlFQJ21X1c/cnlQ+Y3szxYnooYIAEyZtvVgRordMky5TCgwgxoLs64Dylo1ekIFcI1LA+RteRlzlnYSLtfWu3G2CbExUpmb/Lpw3rdVjfK5BLAgdEEHSLMF1SyUCZeA6cgdHGPM+UZLS+132Yck4P79+Wh7NH4LDosshCABNLGTb18WY5xWndhyHGddzklx9/wszLum6Xy8vz08N61luNY8QYlz5gGBGtaRIRyrLenb//4YfH//hOE6npuB0zc5nTtu+G1NWu//yff/7tH2/n+7e//MnHj+89uvzx939Qs5yzq20vl6dPn/ayTTlnyWqmqub27tt3Dw/3jjCfVkkJCLsbMed7itE5weP5zMyE+HmdwmCaH374xc/LzAwuZxEnJiRKPCyw6/c//enHjx9FBEWaDiJc5yUiamtHb4YxszBA672URYmUQsfIFJPk9e78u//6T4o0zUWjJ5kp54wIKcte61/+9ndJ+f2H7+Zl1TAkZ4Jwba0+PX/ScUyJl5xyZmYsiRUGJfJjxHDrbgZgIYhTQUK9W9YpJRGRAJim6euXl1TK/cPD6XQaYwBC7z0iWmvKo+T8uukSs6oiAQO7e8oZIiIi52xmOeWImKYSETmJ6hBEZGYze3z8pqQ5pWRmESHMavr6c7zmE1GtVVUjYlmWWusYo/eeU+q9m9k0Tfu+R1hEqNrtdsuZSE1fj/48zymliAAADweAnHMpZZomVW2tAcC6rjnn17sIAGMMMxtjRIS7v4b9+01xVa21CTP33pnZzYiImVNK7tZ6J8Jaa85lnmdV3batlDKGuvsrwGldzR3gtUkREfM8HcdRa22tMQshkfXx+ccfW629t5QIKQAMMGyoKxDw5eul1jY0kIQ45TLNy+ym9XYDc7JInE/LKacSFlMpACEpkaRjKInQaO28ns7rCSEAAzG22xUxhKXkUvKkwy5fL2aGSIgEgO6mOrR3GwPcj6MSEQK01nprEA4QKWciQgjJZUo5q+oYw9z5XyYpwTRlZn58fPj0+TOGu6mbRoSpIzIzt1aZGZBvt2vOZduudXdAWJb1vK4Q0a43cTOI2LYNCVnkVToAYJYxWj30tt/u7taUZN93Rigly7RAxBGho7a2f/P2g7sTwem03K5fEKEdN2LC8N47DR37vpu7iKgqAJQy7fuu2gHj+fmp1n2epzAbrfWjtnpkKcJZRNzttl97r5LoaDXAchIhPPb96/Ozmy13Z1rXtdZbmAqzm26Xr63eEGBoJwJmSiJhNvpRkhAGUpiPbbscx66jL8scCNfry+XyfNuu7hbhRAChx7Exx/8DGpWSqK/H22EAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x23CDBF97C10>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a selected image to check\n",
    "i=12645\n",
    "print (image_filenames[i])\n",
    "image = images[i]\n",
    "Image.merge('RGB', (\n",
    "    Image.fromarray(np.asarray(np.split(image[0:1024], indices_or_sections=32))),\n",
    "    Image.fromarray(np.asarray(np.split(image[1024:2048], indices_or_sections=32))),\n",
    "    Image.fromarray(np.asarray(np.split(image[2048:3072], indices_or_sections=32)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Groundtruth based on labels:\n",
    "\n",
    "In this data set, all images are labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 25 labels:\n",
      "[6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3, 6, 6, 2]\n",
      "\n",
      "Unique labels:\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "\n",
      "Groundtruth Statistics:\n",
      "Label 0 : 5000\n",
      "Label 1 : 5000\n",
      "Label 2 : 5000\n",
      "Label 3 : 5000\n",
      "Label 4 : 5000\n",
      "Label 5 : 5000\n",
      "Label 6 : 5000\n",
      "Label 7 : 5000\n",
      "Label 8 : 5000\n",
      "Label 9 : 5000\n"
     ]
    }
   ],
   "source": [
    "# look at the first 25 classes\n",
    "print('First 25 labels:\\n' + str(image_labels[0:25]) + '\\n')\n",
    "\n",
    "unique_image_labels = set(image_labels)\n",
    "print('Unique labels:\\n' + str(unique_image_labels) + '\\n')\n",
    "\n",
    "print(\"Groundtruth Statistics:\")\n",
    "for v in unique_image_labels:\n",
    "    print(\"Label\", v, \":\", image_labels.count(v))\n",
    "\n",
    "image_classes = np_utils.to_categorical(image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "Here we use <b>Zero-mean Unit-variance standardization</b> which means we deduct the mean and divide by the standard deviation.\n",
    "\n",
    "(Note: Here, we do this \"flat\", i.e. one mean and std.dev. for the whole image is computed over all pixels (not per pixel); in RGB images, standardization can be done e.g. for each colour channel individually; in other/non-image data sets, attribute-wise standardization should be applied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 255)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.min(), images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120.70756512369792, 64.1500758911213)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = images.mean()\n",
    "stddev = images.std()\n",
    "mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.5247951877342226e-17, 1.0000000000000022)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_std = (images - mean) / stddev\n",
    "images_std = np.array(images_std, dtype=float)\n",
    "images_std.mean(), images_std.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.8816433721538972, 2.09341038199596)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_std.min(), images_std.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating NN Models in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1) Fully Connected NN\n",
    "\n",
    "For a fully connected neural network, the x and y axis of an image do not play a role at all. All pixels are considered as a completely individual input to the neural network. Therefore the 2D image arrays have to be flattened to a vector. For our current data set this is already the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "#  flatten images to vectors\n",
    "#images_flat = images_std.reshape(images_std.shape[0],-1)\n",
    "#print(images_flat.shape)\n",
    "print(images_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape for NN: 3072\n"
     ]
    }
   ],
   "source": [
    "# find out input shape for NN, which is just a long vector (32 x 32 x 3 = 1024 x 3 = 3072)\n",
    "input_shape = images_std.shape[1]\n",
    "print('Input shape for NN: ' + str(input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Model\n",
    "\n",
    "In Keras, one can choose between a Sequential model and a Graph model. Sequential models are the standard case. Graph models are for parallel networks and use the functional API (see Music/Speech tutorial).\n",
    "\n",
    "Here we create a sequential model with 2 fully connected (a.k.a. 'dense') layers containing 256 units each.\n",
    "\n",
    "The output unit is a Single sigmoid unit which can predict values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_37 (Dense)            (None, 256)               786688    \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 855,050\n",
      "Trainable params: 855,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# simple Fully-connected network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_dim=input_shape))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dense(len(unique_image_labels), activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss Function and Optimizer Strategy: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a loss function \n",
    "loss = 'categorical_crossentropy'  # 'categorical_crossentropy' for multi-class problems\n",
    "\n",
    "# Optimizer = Stochastic Gradient Descent\n",
    "optimizer = 'sgd' \n",
    "\n",
    "# Compiling the model\n",
    "# This creates the whole model structure in memory. \n",
    "# If you use GPU computation, here GPU compatible structures and code is generated.\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.8237 - accuracy: 0.3621\n",
      "Epoch 2/7\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6947 - accuracy: 0.4129\n",
      "Epoch 3/7\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6427 - accuracy: 0.4345\n",
      "Epoch 4/7\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6044 - accuracy: 0.4475\n",
      "Epoch 5/7\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5727 - accuracy: 0.4603\n",
      "Epoch 6/7\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5442 - accuracy: 0.4708\n",
      "Epoch 7/7\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5159 - accuracy: 0.4819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23cac67c670>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "epochs = 7\n",
    "model.fit(images_std, image_classes, batch_size=32, epochs=epochs) #, validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 3s 2ms/step\n",
      "Accuracy on training set: 0.49134\n"
     ]
    }
   ],
   "source": [
    "# verify accuracy on train set\n",
    "predictions = model.predict(images_std)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "image_labels_for_comparison = np.array(image_labels)\n",
    "accuracy_on_training_set = accuracy_score(image_labels_for_comparison, predicted_classes)\n",
    "print('Accuracy on training set: ' + str(accuracy_on_training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the accuracy on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 1 files.\n",
      "Imported 10000 images.\n"
     ]
    }
   ],
   "source": [
    "# Unpickle files\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "path = 'cifar-10-batches-py/test_batches'\n",
    "files = glob.glob(os.path.join(path, '*_*'))\n",
    "print('Imported ' + str(len(files)) + ' files.')\n",
    "\n",
    "# Import labeled images\n",
    "test_image_filenames = []\n",
    "test_images = np.empty([0,3072], dtype=np.ubyte)\n",
    "test_image_labels = []\n",
    "\n",
    "for filename in files:\n",
    "    file_data = unpickle(filename)\n",
    "    test_image_filenames.extend(file_data[b'filenames'])\n",
    "    test_images = np.append(test_images, file_data[b'data'], axis=0)\n",
    "    test_image_labels.extend(file_data[b'labels'])\n",
    "    \n",
    "test_image_labels_for_comparison = np.array(test_image_labels)\n",
    "test_image_classes = np_utils.to_categorical(test_image_labels)\n",
    "\n",
    "print('Imported ' + str(len(test_images)) + ' images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Test Set\n",
    "\n",
    "The test data has to be standardized <b>in the same way</b> as the training data for compatibility with the model! That means, we take the mean and standard deviation of the <i>training data</i> to transform also the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NO! we take the same mean and stddev from the training data above!\n",
    "#mean = test_images.mean()\n",
    "#stddev = test_images.std()\n",
    "#print mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121.52915475260417, 64.06097012299574)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.mean(), test_images.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_images_std = (test_images - mean) / stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.012807305642173993, 0.9986109795368484)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_std.mean(), test_images_std.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "Accuracy on test set: 0.4603\n"
     ]
    }
   ],
   "source": [
    "# verify accuracy on test set\n",
    "test_predictions = model.predict(test_images_std)\n",
    "test_predicted_classes = np.argmax(test_predictions, axis=1)\n",
    "accuracy_on_test_set = accuracy_score(test_image_labels_for_comparison, test_predicted_classes)\n",
    "print('Accuracy on test set: ' + str(accuracy_on_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Convolutional Neural Network\n",
    "\n",
    "A Convolutional Neural Network (ConvNet or CNN) is a type of (deep) Neural Network that is well-suited for 2D axes data, such as images, as it is optimized for learning from spatial proximity. Its core elements are 2D filter kernels which essentially learn the weights of the Neural Network, and downscaling functions such as Max Pooling.\n",
    "\n",
    "A CNN can have one or more Convolution layers, each of them having an arbitrary number of N filters (which define the depth of the CNN layer), following typically by a pooling step, which groups neighboring pixels together and thus reduces the image resolution by retaining only the maximum values of neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "Our input to the CNN is the standardized version of the original image array.\n",
    "\n",
    "#### Adding the channel\n",
    "\n",
    "For CNNs, we need to add a dimension for the color channel to the data. RGB images typically have an 3rd dimension with the color. \n",
    "<b>For greyscale images we need to add an extra dimension for compatibility with the CNN implementation.</b>\n",
    "\n",
    "In Theano, traditionally the color channel was the <b>first</b> dimension in the image shape. \n",
    "In Tensorflow, the color channel is the <b>last</b> dimension in the image shape. \n",
    "\n",
    "This can be configured now in ~/.keras/keras.json: \"image_dim_ordering\": \"th\" or \"tf\" with \"tf\" (Tensorflow) being the default image ordering even though you use Theano. Depending on this, use one of the code lines below.\n",
    "\n",
    "For greyscale images, we add the number 1 as the depth of the additional dimension of the input shape (for RGB color images, the number of channels is 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50000, 1024, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "image = images[i]\n",
    "new_image = Image.merge('RGB', (\n",
    "    Image.fromarray(np.asarray(np.split(image[0:1024], indices_or_sections=32))),\n",
    "    Image.fromarray(np.asarray(np.split(image[1024:2048], indices_or_sections=32))),\n",
    "    Image.fromarray(np.asarray(np.split(image[2048:3072], indices_or_sections=32)))))\n",
    "\n",
    "reshaped_images = []\n",
    "for image in images_std:\n",
    "    reshaped_image =  images_std[i].reshape(1024, 1, 3)\n",
    "    reshaped_images.append(reshaped_image)\n",
    "\n",
    "reshaped_images = np.array(reshaped_images)\n",
    "print(type(reshaped_images))\n",
    "print(reshaped_images.shape)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif keras.backend.image_data_format.c [\\'channels_first\\'] == \\'th\\':\\n    # Theano ordering (~/.keras/keras.json: \"image_dim_ordering\": \"th\")\\n    train_img = images_std.reshape(images_std.shape[0], n_channels, edge_pixels, edge_pixels)\\n    test_img = test_images_std.reshape(test_images_std.shape[0], n_channels, edge_pixels, edge_pixels)\\nelse:\\n    # Tensorflow ordering (~/.keras/keras.json: \"image_dim_ordering\": \"tf\")\\n    train_img = images_std.reshape(images_std.shape[0], edge_pixels, edge_pixels, n_channels)\\n    test_img = test_images_std.reshape(test_images_std.shape[0], edge_pixels, edge_pixels, n_channels)\\n    '"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_channels = 3 # for grey-scale, 3 for RGB, but usually already present in the data\n",
    "edge_pixels = 32\n",
    "\n",
    "keras.backend.set_image_data_format('channels_last')\n",
    "train_img = images_std.reshape(images_std.shape[0], edge_pixels, edge_pixels, n_channels)\n",
    "test_img = test_images_std.reshape(test_images_std.shape[0], edge_pixels, edge_pixels, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(10000, 3072)\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(images_std.shape)\n",
    "print(test_images_std.shape)\n",
    "print(train_img.shape)\n",
    "print(test_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of images)\n",
    "    \n",
    "input_shape = train_img.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN model\n",
    "\n",
    "You may try to change the following to see the impact on the result:\n",
    "* number of filters\n",
    "* filter kernel size (e.g. 3 x 3, 5 x 5, ...)\n",
    "* adding/not adding Batch Normalization\n",
    "* adding/not adding ReLU Activation\n",
    "* adding/not adding Max Pooling\n",
    "* changing Pooling size (e.g. 1 x 2, 2 x 2, 2 x 1, or more)\n",
    "* adding/changing/removing Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createMyModel():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    n_filters = 16 #16\n",
    "    # this applies n_filters convolution filters of size 5x5 resp. 3x3 each in the 2 layers below\n",
    "\n",
    "    # Layer 1\n",
    "    #model.add(Convolution2D(n_filters, 3, 3, border_mode='valid', input_shape=input_shape))\n",
    "    model.add(Convolution2D(n_filters, 3, 3, input_shape=input_shape))\n",
    "    # input shape: 100x100 images with 3 channels -> input_shape should be (3, 100, 100) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))  # ReLu activation\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))) # reducing image resolution by half\n",
    "    model.add(Dropout(0.3))  # random \"deletion\" of %-portion of units in each batch\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Convolution2D(n_filters, 3, 3))  # input_shape is only needed in 1st layer\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten()) # Note: Keras does automatic shape inference.\n",
    "    \n",
    "    # Full Layer\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(len(unique_image_labels), activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_7 (Conv2D)           (None, 10, 10, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 10, 10, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 10, 10, 16)        0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 5, 5, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 5, 5, 16)          0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 1, 1, 16)          2320      \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 1, 1, 16)          0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 1, 1, 16)          0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 256)               4352      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,754\n",
      "Trainable params: 9,722\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = createMyModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you get OverflowError: Range exceeds valid bounds in the above box, check the correct Theano vs. Tensorflow ordering in the box before and your keras.json configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "loss = 'categorical_crossentropy' \n",
    "optimizer = 'sgd' \n",
    "#optimizer = SGD(lr=0.001)  # possibility to adapt the learn rate\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.2351 - accuracy: 0.1569\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1480 - accuracy: 0.2070\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0984 - accuracy: 0.2287\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0506 - accuracy: 0.2471\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0170 - accuracy: 0.2581\n"
     ]
    }
   ],
   "source": [
    "# TRAINING the model\n",
    "epochs = 5\n",
    "history = model.fit(train_img, image_classes, batch_size=32, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Validation Data\n",
    "\n",
    "We split off 10 % of the training data and use it as independend validation set to verify how good we are\n",
    "on an independent data (not used for training) in each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 7s 4ms/step - loss: 2.2517 - accuracy: 0.1490 - val_loss: 2.1479 - val_accuracy: 0.2294\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 2.1639 - accuracy: 0.1965 - val_loss: 2.0532 - val_accuracy: 0.2748\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 2.1040 - accuracy: 0.2208 - val_loss: 1.9782 - val_accuracy: 0.3002\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 2.0631 - accuracy: 0.2389 - val_loss: 1.9283 - val_accuracy: 0.3314\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 2.0323 - accuracy: 0.2545 - val_loss: 1.9098 - val_accuracy: 0.3354\n"
     ]
    }
   ],
   "source": [
    "# recreate and recompile the model (otherwise we continue learning)\n",
    "model = createMyModel()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# train with showing accuracy on split off validation data\n",
    "history = model.fit(train_img, image_classes, batch_size=32, epochs=epochs, validation_split=0.1) # portion of val. data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on the split-off validation data are quite high (usually similar, but not as high as on the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Test Set as Validation Set\n",
    "\n",
    "<b>Note: This usually is not recommended as during experimentation you will overfit also to the test data.</b>\n",
    "\n",
    "We show it here only for demonstration purposes to see how (bad) the validation accuracy is on our independet test data. The recommended way is to have a separate training, validation and test set (i.e. 3 splits or separate data sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.2446 - accuracy: 0.1503 - val_loss: 2.1315 - val_accuracy: 0.2512\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 2.1400 - accuracy: 0.2076 - val_loss: 2.0108 - val_accuracy: 0.2960\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0804 - accuracy: 0.2325 - val_loss: 1.9654 - val_accuracy: 0.3090\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0411 - accuracy: 0.2528 - val_loss: 1.9151 - val_accuracy: 0.3279\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 2.0090 - accuracy: 0.2670 - val_loss: 1.9016 - val_accuracy: 0.3311\n"
     ]
    }
   ],
   "source": [
    "# recreate and recompile the model (otherwise we continue learning)\n",
    "model = createMyModel()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# show result on Test Data while training \n",
    "# we use test data as validation data to see direct results (usually NOT RECOMMENDED due to overfitting to the problem!)\n",
    "#validation_data = (test_img, test_image_labels_for_comparison)\n",
    "validation_data = (test_img, test_image_classes)\n",
    "\n",
    "history = model.fit(train_img, image_classes, batch_size=32, epochs=epochs, validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n",
      "Accuracy on test set: 0.3311\n"
     ]
    }
   ],
   "source": [
    "# verify accuracy on test set\n",
    "predictions = model.predict(test_img)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "accuracy_on_test_set = accuracy_score(test_image_labels_for_comparison, predicted_classes)\n",
    "print('Accuracy on test set: ' + str(accuracy_on_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Training Curve\n",
    "\n",
    "The `model.fit` function returns a history including the evolution of training and validation loss and accuracy. We can plot it to see a nice training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = history.history\n",
    "hist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23c9f511940>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAGDCAYAAAAPl5VaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxTElEQVR4nO3de3TV9Z3v/9d7X3KBBEi430EPFSso2nihcwapnh+1/aFOrRdaaytrCj8vtcLMOJ6xrXBO6cwcj2Ntjx4sbZVSbdWl0l+XWhyoVWqPOgKi2GppiyIoSiIhEMh178/5Y++EnWTfks9O9s7O87HWd+V7+Xxv+brIy8/n8/18zTknAAAA9E0g3xcAAAAwmBGmAAAAPBCmAAAAPBCmAAAAPBCmAAAAPBCmAAAAPBCmAAwoM/uVmX0l12UBIF+McaYAZGJmjQmLwyS1SIrEl/8/59xDA39VfsxshKT/LukySdWSPpD0pKQ1zrm6fF4bgMGFmikAGTnnKjomSe9KujhhXWeQMrNQ/q4ye2ZWIunXkk6TdJGkEZI+KekjSef04XiD4r4B9A/CFIA+M7OFZrbfzG41sw8kPWBmVWb2pJnVmll9fH5Kwj7PmdlX4/PXmtkLZnZnvOzbZvaZPpadaWZbzeyomW0xs3vN7MEUl/5lSdMkfc459wfnXNQ5d9A5923n3NPx4zkz+08Jx19vZmvS3PebZrY4oXzIzOrM7Kz48nlm9n/M7LCZvWZmCz1//QAKBGEKgK8JijWTTZe0XLF/Vx6IL0+T1CTpnjT7nyvpj5LGSLpD0o/NzPpQ9meS/kPSaEmrJV2T5pz/RdIm51xjmjKZdL/vn0v6QsL2T0uqc87tMLPJkp6StCa+zz9IetzMxnqcH0CBIEwB8BWVtMo51+Kca3LOfeSce9w5d9w5d1TSdySdn2b/vc65HzrnIpJ+ImmipPG9KWtm0ySdLel251yrc+4FSb9Mc87Rkg707jZ76HLfioW5S8xsWHz7F+PrJOlLkp52zj0drwXbLGmbpM96XgOAAkCYAuCr1jnX3LFgZsPM7AdmttfMjkjaKmmUmQVT7P9Bx4xz7nh8tqKXZSdJOpSwTpL2pbnmjxQLYj663Ldz7s+S3pR0cTxQXaITYWq6pCviTXyHzeywpP+cg2sAUADoNAnAV/dXgv9e0imSznXOfWBm8yS9KilV010uHJBUbWbDEgLV1DTlt0haY2bDnXPHUpQ5rtibix0mSNqfsJzsVeiOpr6ApD/EA5YUC3Y/dc4ty3AfAAYhaqYA5FqlYv2kDptZtaRV/X1C59xexZrNVptZiZnNl3Rxml1+qljAedzMZptZwMxGm9ltZtbR9LZT0hfNLGhmFyl9U2WHhyUtknS9TtRKSdKDitVYfTp+vLJ4J/YpSY8CYFAhTAHItbsllUuqk/SSpE0DdN6rJc1XrAlvjaRHFBsPqwfnXItindDfkrRZ0hHFOq+PkfRyvNjNigWyw/Fj/yLTBTjnDkh6UbFhFh5JWL9P0qWSbpNUq1iQu0X8GwwUBQbtBFCUzOwRSW855/q9ZgzA0Mb/FQEoCmZ2tpmdHG+yu0ixmqBf5PmyAAwBdEAHUCwmSHpCsWEP9ku63jn3an4vCcBQQDMfAACAB5r5AAAAPBCmAAAAPOStz9SYMWPcjBkz8nV6AACArG3fvr3OOZf0e5p5C1MzZszQtm3b8nV6AACArJnZ3lTbaOYDAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJjqpqKiIt+XAAAABhHCFAAAgAfCVArOOd1yyy2aM2eO5s6dq0ceeUSSdODAAS1YsEDz5s3TnDlz9Nvf/laRSETXXnttZ9nvfve7eb56AAAwUPL2OZmMVqyQdu7M7THnzZPuvjurok888YR27typ1157TXV1dTr77LO1YMEC/exnP9OnP/1pfeMb31AkEtHx48e1c+dOvffee3rjjTckSYcPH87tdQMAgIJVvDVTkYj00UdSY6PU3t7r3V944QV94QtfUDAY1Pjx43X++efrlVde0dlnn60HHnhAq1ev1q5du1RZWamTTjpJe/bs0U033aRNmzZpxIgR/XBDAACgEBVuzVSWNUgpbd8u1dScWK6okGprpc98Rpo2res0dao0ZYpUUtJZ3DmX9LALFizQ1q1b9dRTT+maa67RLbfcoi9/+ct67bXX9Mwzz+jee+/Vo48+qvvvv9/v+gEAwKBQuGHK16mnSi+9JO3bJ737btdp+/ZYsEpkJk2YIDU1SVdeqQUtLfrBM8/oKyNH6tCoUdr63HP6n3fcob1792ry5MlatmyZjh07ph07duizn/2sSkpK9PnPf14nn3yyrr322rzcMgAAGHjFG6aGDZPOPTc2JdPUlDxobdggvfaaPrd3r15sadEZl18uk3SHpAknn6yfjByp/3n0qMKlpaqoqNCGG2/Ue088oaXf+56ioZBkpn/5l38ZyDsFAAB5ZKmas/pbTU2N27ZtW17OnRXnpLq6nmErcfrgg577jRvXsxkxsTlx3DgpULxd1QAAKEZmtt05V5NsW/HWTPkyk8aOjU2f+ETyMi0t0nvvJQ9ab74pPfOMdOxY131KSmKhKl3gGj68/+8PAADkBGHKR2mpdNJJsSkZ56T6+li4Stak+OtfS++/L0WjXfcbPTp94JowQQoG+//+AABARoSp/mQmVVfHpnnzkpdpa4sFqu5Ba98+6e23peeflxoauu4TCsXePkxWq9Uxz/AMAAAMCMJUvoXD0vTpsSmVhobkNVvvviv99rexpsbuY2mNHJm6ZmvaNGnSpFgoAwAAXvhrOhiMHBmb5sxJvj0SiXWGT9VR/sUXpUOHuu4TCMQCVbrANWpUrHYNAACkRJgqBsGgNHlybJo/P3mZxsZY7VayGq5XXpGeeEJqbe26T0VF6mbEadN6DHQKAMBQRJgaKioqYgOZnnpq8u3RqHTwYM9+Wx3zO3bEtifqGOg0Xe3W6NHUbgEAihphKk/a29sVKqQ+S4FALBhNmCCdc07yMk1N0v79yTvLv/669OSTsTKJysszDwVRVtb/9wcAQD8poL/mheNv/uZvtG/fPjU3N+vmm2/W8uXLtWnTJt12222KRCIaM2aMfv3rX6uxsVE33XSTtm3bJjPTqlWr9PnPf14VFRVqbGyUJD322GN68skntX79el177bWqrq7Wq6++qrPOOktXXXWVVqxYoaamJpWXl+uBBx7QKaecokgkoltvvVXPPPOMzEzLli3Txz/+cd1zzz3auHGjJGnz5s1au3atnnjiiYH7xZSXS7NmxaZknIt9XDpV361f/SrWt6v7QLHdBzrtHr4Y6BQAUMAKNkytWCHt3JnbY86bl933k++//35VV1erqalJZ599ti699FItW7ZMW7du1cyZM3Uo3pn729/+tkaOHKldu3ZJkurr6zMee/fu3dqyZYuCwaCOHDmirVu3KhQKacuWLbrtttv0+OOPa926dXr77bf16quvKhQK6dChQ6qqqtKNN96o2tpajR07Vg888ICWLl3q8dvoB2bSmDGx6ayzkpdpbU090Olbb/V+oNOO9Qx0CgDIk4INU/n0/e9/v7MGaN++fVq3bp0WLFigmTNnSpKqq6slSVu2bNHDDz/cuV9VVVXGY19xxRUKxgfcbGho0Fe+8hX96U9/kpmpra2t87jXXXddZzNgx/muueYaPfjgg1q6dKlefPFFbdiwIUd3PIBKSqSZM2NTMs5Jhw8n77f17rvSs8/Gwlj3gU6rq3t2jq+ulqqqek4jRlDTBQDImYINU9nUIKUTOR7R4a2HVX5yucqmlylQkt0fz+eee05btmzRiy++qGHDhmnhwoU644wz9Mc//rFHWeecLEnn6sR1zc3NXbYNT6hB+da3vqVPfepT2rhxo9555x0tXLgw7XGXLl2qiy++WGVlZbriiisKq89VrpidCD1nnJG8THt76oFO33lH2ro1FshSCQRiQ00kC1rppupqghgAoIci/Gscc/zN49r1mVjzmwJS6dRSlZ9UHgtXJ5Wd+HlSucLV4c79GhoaVFVVpWHDhumtt97SSy+9pJaWFj3//PN6++23O5v5qqurtWjRIt1zzz26O5786uvrVVVVpfHjx+vNN9/UKaecoo0bN6qysjLpNTY0NGjy5MmSpPXr13euX7Roke677z4tXLiws5mvurpakyZN0qRJk7RmzRpt3ry5X35vg0IodKIGKpVjx2Kf8slmOnQoFsQ6luM1hEmZxcbf6m0Qq6qKBTiCGAAUnaINU8NmD9O8rfPUvKdZTX9pUtOeJjX/pVl1v6xT28GufyxDo0KdAWvWtFk69vYxnXbSaTpl9ik679zzNHbsWK1bt06XXXaZotGoxo0bp82bN+ub3/ymbrzxRs2ZM0fBYFCrVq3SZZddpn/913/V4sWLNXXqVM2ZM6ezM3p3//iP/6ivfOUruuuuu3TBBRd0rv/qV7+q3bt36/TTT1c4HNayZcv0ta99TZJ09dVXq7a2Vh//+Mf775dXDIYPj01TpvRuP+ek48ezD2L19bE3HDvmu4/VlcisbzViHUGM7zECQEEy1/3NqgFSU1Pjtm3blpdztx9tV/PbzZ0Bq2lPk5r+0qTmPc1qfqdZru3E78RCprIZZV1rs04uV/lJsflQ5cDm0a997Ws688wz9bd/+7cDel5kwbnY0BC9CWKJU0tL6mObxZoYkzU9EsQAoN+Z2XbnXE3SbUMxTKXjIk4t+1u61GYlhq32+q7fwAuPDXcNWCeXdTYnlkwskQVyN2DlJz7xCQ0fPlybN29WaWlpzo6LAtHUFGtyzHUQk/peIzZqFEEMAESYyqm2+rauTYfx+eY9zWp+t1lKeMksUBZQ2cyypGGrbGaZguX8kUKO+NSIdXtJoodkNWKpOugTxAAUqXRhqmj7TPWXcFVY4U+EVfmJnp3Ko61RNb/b3KM2q+kvTWp4vkGRxkiX8iWTSno2HcbDVnhsOOkbfUBS5eWxadKk3u/b3Jy8U36q8PXWW7kPYslqxIrxbVUARYmaqQHinFNbbVuXpsPEGq7W97p2XA5WBDvfNkxsOiw7qaxXQz0A/SpZEMt26v7poe4qK/sexMLh9McGgF6iZqoAmJlKxpWoZFyJRp43ssf2SFNEze80d6nNat7TrON/PK6PfvWRXEtC6O0Y6qF702G8hitcxR8SDJCyMmnixNjUWy0tvRu+Yvfu7INYOBx7m7Oi4sSbndksZ1OGpksA3RCmCkSwPKjhpw7X8FN7fhbFRZ1aD7R2bTrMNNRDt9qszlqtqWWyIM2HKAClpSc+rt1b6YJYQ0NsnLGOqbHxxHxdXWxg18TtmZoquysr630Ayya0DRvGOGTAIEWYGgQsYCqdXKrSyaUa9dejemzvHOrhL12bDhtfbVTdxjq59oShHsKmsullKcNWqIL/JDAI+ASx7iKR1OEr1bpkywcO9NyebgDYZIYN86s1S7VPWVlseA0A/YK/nB4qKipSDsj5zjvvaPHixXrjjTf6/TpClSFVnF6hitMremxzEafmfc1dmg47wtbRl4+q/XC3oR7GhZM2HZaflPuhHoCCEAzGOsqPGJH7Y7e19S6QpVo+dKjnukgk8/k7mPVPSBs+PPa9TYIahjjCVJGzoKl8RrnKZ5Sr6oKqHtt7DPUQ7xzf8EKDDv78YNKhHnp8kufkcpXNYKgHoIdwONYhftSo3B7Xudho+70NaMnWffhhzzK9eTEpGMxdf7Tuy7xIgEGiYMPUn1b8SY07k9f69FXFvArNuntWyu233nqrpk+frhtuuEGStHr1apmZtm7dqvr6erW1tWnNmjW69NJLe3Xe5uZmXX/99dq2bZtCoZDuuusufepTn9Lvf/97LV26VK2trYpGo3r88cc1adIkXXnlldq/f78ikYi+9a1v6aqrrvK673QyDvWwtzlp2Kr/Tb2ix6JdypdMLulRm8VQD0A/MIs1dZaWSqNH5/bYHSP5+zZ7dnxqKXF7phcHuguHexfAhg2LDRHS8TPZ1H1bOEzNGrwVbJjKhyVLlmjFihWdYerRRx/Vpk2btHLlSo0YMUJ1dXU677zzdMkll/QqGNx7772SpF27dumtt97SokWLtHv3bt133326+eabdfXVV6u1tVWRSERPP/20Jk2apKeeekpS7GPI+RIoCWjYrGEaNmtYj23JhnroaEas31yvD3/yYZfynUM9JBlXq2waQz0ABcMsFjiGDZPGjs3tsaPR2LcvfZs9a2tjLxIkrsv0FYBUAoH0Aaw3wSyb9dS2FaWCDVPpapD6y5lnnqmDBw/q/fffV21traqqqjRx4kStXLlSW7duVSAQ0HvvvacPP/xQE3rR8fWFF17QTTfdJEmaPXu2pk+frt27d2v+/Pn6zne+o/379+uyyy7TrFmzNHfuXP3DP/yDbr31Vi1evFh//dd/3V+36yWroR6SfP/w+FvH9dHTPYd6KJuW4vuHJ5cpPIp/fICiEAjEapIqKqTx43N77Pb2WFBrauo5pVqfblvH+o6hOLqv702ftUTBYG6CWbb7EN4GRMGGqXy5/PLL9dhjj+mDDz7QkiVL9NBDD6m2tlbbt29XOBzWjBkz1NzLV6lTDYz6xS9+Ueeee66eeuopffrTn9aPfvQjXXDBBdq+fbuefvpp/dM//ZMWLVqk22+/PRe3NqCC5UEN//hwDf94mqEeknySp+4XdWqr7TbUQ1Uo5fcPS6eUMtQDgNiI+f31IkEybW29D2bZbDt0KPn6XIW3vtSm9SbMDdHwljFMmdlUSRskTVCsO/I659z3upUxSd+T9FlJxyVd65zbkfvL7X9LlizRsmXLVFdXp+eff16PPvqoxo0bp3A4rN/85jfau3dvr4+5YMECPfTQQ7rgggu0e/duvfvuuzrllFO0Z88enXTSSfr617+uPXv26PXXX9fs2bNVXV2tL33pS6qoqND69etzf5N51mWohwWjemxvP9reOZZW4huIjTsaVfdEkqEeZvT8/mHZ9DKFRoYUrAwqWBlUoDRAny0AuRMOx6Z8hLfehrZ0Ye6jj5KXj0YzX1MywWBum0Yz7VMgn53K5iraJf29c26HmVVK2m5mm51zf0go8xlJs+LTuZLWxn8OOqeddpqOHj2qyZMna+LEibr66qt18cUXq6amRvPmzdPs2bN7fcwbbrhB1113nebOnatQKKT169ertLRUjzzyiB588EGFw2FNmDBBt99+u1555RXdcsstCgQCCofDWrt2bT/cZWELVYZUcUaFKs7oOdRDtD2qlv0tST/Jk2yohw4Wss5gFawMKlgRVKgy1GVdb5YDYfp4ARhAAxnenEte8+ZbA3f8eM/w1rFPX8NbKBQLVsuXS3femdvfQy/0+tt8Zvb/S7rHObc5Yd0PJD3nnPt5fPmPkhY65w6kOs5Q+zYfBkZbfZua/tKklndbFDkaUfvRdkWORjqnTMuJtV7pWKl5hbEuyxVBxu8CMHQlC2+97ec2f7505ZX9epk5+zafmc2QdKakl7ttmixpX8Ly/vi6LmHKzJZLWi5J06ZN682pgayEq8IK14SlpP+5p+ecU7Ql2qvwlbjcVtemprebTmxvjEhZ/r9KYHigS7jyCWiBYTRpAhhEzGKDv5aUSCN7vtA0GGQdpsysQtLjklY4545035xklx5/Rpxz6yStk2I1U724zoK1a9cuXXPNNV3WlZaW6uWXu+dNFDozU7AsqGBZUMrBG+HOOUWORZKHr8bMAa31/VY1HW3qXI4ez7IaPKDOQJaL2rNAKU2aAJBOVmHKzMKKBamHnHNPJCmyX9LUhOUpkt73v7zCN3fuXO3cuTPfl4ECZGYKVYRi3zuc6H88F3GKNPa+6bJjua22rcuya82ySTOc0N/Ms69ZsDKoQIhwBqC4ZPM2n0n6saQ3nXN3pSj2S0lfM7OHFet43pCuvxSA3rOgKTQypNDI3Ly9Em2NZg5jjSkC2pF2tbzX0mUfZfnmdqAskJu+ZpVBBYfT3wxA/mXzr/JfSbpG0i4z2xlfd5ukaZLknLtP0tOKDYvwZ8WGRlia8ysFkFOBkoACowMKj/YfF8Y5p2hztNe1ZR1T68FWRf4S6RListXZx6yjaXNESMERwdjQGPGfKdeNPFGeGjMAfZUxTDnnXlDyPlGJZZykG3N1UQAGFzNTsDwY+9j1OP/juWjX/ma9qj07ElHzO82KHImovaFd7UeyqzULDAv0CFjJQlfadZXUlAFDUWGMdgUACSwQG3oiVOn/T5RzTtHjUbUfae8SsDrm0607/sHxE+uOZvd2ZrAymDl0Zag5Cw4P8kYmMIgQpjxUVFSosbEx35cBIA0zU3B4LKD4vAjgovEXALIJYkfaT8zXt6tlb0vnuuixLN7KDKh3zZUp1gXKGCYDGAiEqSLQ3t6uUIEMqQ8UKwuYQiNiQcVHtD3a2RzZJXQlC2JH2hVpiP1s/bBVTX9q6lwXbc4cyixsfQ5iiesY8R9Ir2D/Aq/YtEI7P9iZ02POmzBPd190d8rtt956q6ZPn64bbrhBkrR69WqZmbZu3ar6+nq1tbVpzZo1uvTSSzOeq7GxUZdeemnS/TZs2KA777xTZqbTTz9dP/3pT/Xhhx/quuuu0549eyRJa9eu1aRJk7R48WK98cYbkqQ777xTjY2NWr16tRYuXKhPfvKT+t3vfqdLLrlEH/vYx7RmzRq1trZq9OjReuihhzR+/Hg1Njbqpptu0rZt22RmWrVqlQ4fPqw33nhD3/3udyVJP/zhD/Xmm2/qrrtSvawJIFcCoYACVQGFq/w6/kdbE5ouE0JXsiCWWIPWsq+lyzrXlrntMlAW8K4lC40I8VFyFK2CDVP5sGTJEq1YsaIzTD366KPatGmTVq5cqREjRqiurk7nnXeeLrnkkoxV52VlZdq4cWOP/f7whz/oO9/5jn73u99pzJgxOnTokCTp61//us4//3xt3LhRkUhEjY2Nqq+vT3uOw4cP6/nnn5ck1dfX66WXXpKZ6Uc/+pHuuOMO/du//Zu+/e1va+TIkdq1a1dnuZKSEp1++um64447FA6H9cADD+gHP/iB768PwAAKlARUMqZEGtP3Y3SO+p8kdCVdl1CD1vSXpi7rlE3r5fCAdy0Zw2GgEBVsmEpXg9RfzjzzTB08eFDvv/++amtrVVVVpYkTJ2rlypXaunWrAoGA3nvvPX344YeaMGFC2mM553Tbbbf12O/ZZ5/V5ZdfrjFjYv8CVldXS5KeffZZbdiwQZIUDAY1cuTIjGHqqquu6pzfv3+/rrrqKh04cECtra2aOXOmJGnLli16+OGHO8tVVVVJki644AI9+eSTOvXUU9XW1qa5c+f28rcFYLBLHPW/ZHxJn4/T2ck/2879CTVoLe+1nKhJO5LFa5cW6+SfLHQFhwVlIesyKage6yxksmCSdenWp9nWq3MEjTBYhAo2TOXL5Zdfrscee0wffPCBlixZooceeki1tbXavn27wuGwZsyYoebm5ozHSbWfcy7rDqGhUEjRhC9pdz/v8OHDO+dvuukm/d3f/Z0uueQSPffcc1q9erUkpTzfV7/6Vf3zP/+zZs+eraVLGRYMQN8ldvIvnVTa5+N0dvJP1n8sTZ+ytkOx72JGm6JyESfX3nNSRFl/yLzfBdTrMJcysPUlGKYJjDkLn9mcI2hF84IEYaqbJUuWaNmyZaqrq9Pzzz+vRx99VOPGjVM4HNZvfvMb7d27N6vjNDQ0JN3vwgsv1Oc+9zmtXLlSo0eP1qFDh1RdXa0LL7xQa9eu1YoVKxSJRHTs2DGNHz9eBw8e1EcffaSKigo9+eSTuuiii1Keb/LkyZKkn/zkJ53rFy1apHvuuUd33323pFgzX1VVlc4991zt27dPO3bs0Ouvv+7xGwOA3MhVJ/9UnHNSVD3DVooAlnZbmn06gltOzpNiW7Q5mvo8WVx3Ns2yAyIxvHnUFo6+ZLSm3DQlb7dBmOrmtNNO09GjRzV58mRNnDhRV199tS6++GLV1NRo3rx5mj17dlbHSbXfaaedpm984xs6//zzFQwGdeaZZ2r9+vX63ve+p+XLl+vHP/6xgsGg1q5dq/nz5+v222/Xueeeq5kzZ6Y99+rVq3XFFVdo8uTJOu+88/T2229Lkr75zW/qxhtv1Jw5cxQMBrVq1SpddtllkqQrr7xSO3fu7Gz6A4BiZhavFQma1PcKtKLgokkC10AFxL6cJ81+0dZoVm+39ieLDV4+8Gpqaty2bdvycm7ELF68WCtXrtSFF16Y70sBAKCgmdl251xNsm0MHjIEHT58WB/72MdUXl5OkAIAwBPNfJ527dqla665psu60tJSvfzyy3m6osxGjRql3bt35/syAAAoCoQpT3PnztXOnTvzfRkAACBPaOYDAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwkDFMmdn9ZnbQzN5IsX2hmTWY2c74dHvuLxMAAKAwhbIos17SPZI2pCnzW+fc4pxcEQAAwCCSsWbKObdV0qEBuBYAAIBBJ1d9puab2Wtm9iszOy1HxwQAACh42TTzZbJD0nTnXKOZfVbSLyTNSlbQzJZLWi5J06ZNy8GpAQAA8su7Zso5d8Q51xiff1pS2MzGpCi7zjlX45yrGTt2rO+pAQAA8s47TJnZBDOz+Pw58WN+5HtcAACAwSBjM5+Z/VzSQkljzGy/pFWSwpLknLtP0uWSrjezdklNkpY451y/XTEAAEAByRimnHNfyLD9HsWGTgAAABhyGAEdAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAA2EKAADAQ8YwZWb3m9lBM3sjxXYzs++b2Z/N7HUzOyv3lwkAAFCYsqmZWi/pojTbPyNpVnxaLmmt/2UBAAAMDhnDlHNuq6RDaYpcKmmDi3lJ0igzm5irCwQAAChkuegzNVnSvoTl/fF1AAAARS8XYcqSrHNJC5otN7NtZrattrY2B6cGAADIr1yEqf2SpiYsT5H0frKCzrl1zrka51zN2LFjc3BqAACA/MpFmPqlpC/H3+o7T1KDc+5ADo4LAABQ8EKZCpjZzyUtlDTGzPZLWiUpLEnOufskPS3ps5L+LOm4pKX9dbEAAACFJmOYcs59IcN2J+nGnF0RAADAIMII6AAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB4IUwAAAB6yClNmdpGZ/dHM/mxm/zXJ9oVm1mBmO+PT7bm/VAAAgMITylTAzIKS7pX0/0jaL+kVM/ulc+4P3Yr+1jm3uB+uEQAAoGBlUzN1jqQ/O+f2OOdaJT0s6dL+vSwAAIDBIZswNVnSvoTl/fF13c03s9fM7FdmdlqyA5nZcjPbZmbbamtr+3C5AAAAhSWbMGVJ1rluyzskTXfOnSHpf0n6RbIDOefWOedqnHM1Y8eO7dWFAgAAFKJswtR+SVMTlqdIej+xgHPuiHOuMT7/tKSwmY3J2VUCAAAUqGzC1CuSZpnZTDMrkbRE0i8TC5jZBDOz+Pw58eN+lOuLBQAAKDQZ3+ZzzrWb2dckPSMpKOl+59zvzey6+Pb7JF0u6Xoza5fUJGmJc657UyAAAEDRsXxlnpqaGrdt27a8nBsAAKA3zGy7c64m2TZGQAcAAPBAmAIAAPBAmAIAAPBAmAIAAPBAmAIAAPBAmAIAAPBAmAIAAPBAmAIAAPBAmAIAAPBAmAIAAPBAmAIAAPBAmAIAAPBAmAIAAPBAmAIAAPBAmAIAAPBAmAIAAPBAmAIAAPAQyvcFAABQqJxzcnJyzsWW4/PZ/Oy+f6afHL/vx589ZrbmT50/QP9V9ESYAgAkFXVRtUXa1BppVVu0rct8a6RVbZG2LvM5LdeHfSMuktM/5Bg8rq+5njAFAMXIOaeIiwxsCIm0qTWam3NEXKRffz8mU0mwRCXBEoWDYYUD4c75kmCJwoFwl/myUJkqSyuTlwuEFQwEZTKZWVY/JWVd1sw6r7m/jl+I1zRYjl9ZUtmv/61mQpgCUDCiLqpINKL2aHvnFHEnlrPZ1hEEMoWFjGElmpug099CgVCXcJEpkFSUVKQul2HfdOfI9vyJ5YKBYL//foCBQJgC8qR7cOhtaBjQba49adnEcrnYlq+mle5/6DOFgOHh4T3LBfoWQnzLdfzfPYD8IUxhUGqPtutoy1EdbT2qoy1H1dja2Dl/tPWojrUe63ugcP0fGvIZHJIJWEChQEhBCyoUCHVOwcCJ5UzbwoGwykPlmfdLWE4sl8ttnU1HWYSVoAUJJAC8EKYwIDKFn6TrWuPrEsp0rGtub+7ztQQs4BUaOubLQmV5Dw252kaYAIC+I0whqcTwkyrQJF2XIiBlG34CFlBlSaUqSipUWVqpypJKVZZWasywMbF18eWOn6nWdTTDdA8NwUBQAWN4NQBA7hCmikRH+MmqxidxXYGEn4758lA5tSQAgEGFMJUn7dH2pLU7vQk/ifsTfgAAg4VzUjQqtbcnn9raUm9LNk2bJp1+ev7uhzCVpVThJ+O6FAGJ8AMAQ1uyQNHbEJGLIJLr/bM9Ri5df730v/93bo/ZG0UbpqIuqiMtR3LW6ZnwAwADJxqVWltPTG1tXZeTrRssIaK/AkVfhELpp3A4/fayMr/9s5myOca4cXn+Peb39P3n1QOvquaHNRnLBSyQNNBMHzU9Np+wvntASraO8AOgEEUifQsnmcr017pI/w6+3oVvCEgWKAYqRPjsHwhI/LnKjaINU9NHTdddi+4i/ADIOecGXziJRvvnd2EmlZScmMLhrsvd1w0bll25vqxLDBDZBhECBXKhaMPUmGFjtHL+ynxfBoA+iESk5mappSX1lGp79yDRX+HE9dOYq2ZSaWn2YaKiov/CSaZ14bAUDBJGgKINUwCyF4lkF1RytT1TmVw28QSD2YeEsjJpxIj+CR3Z7hfkc3XAoEOYAvKge3gZqJCSanuuwktHrUrHVFbWdbljqqhIvz2bY6TaXlJy4mc4HGvGAYD+RJjCkJAqvOSrNqa/wkuqkNERXnoTUvoSZEIhmnwADD2EKRS0SERqaJDq66XDh2NTx3yydR3zR450DTe5fAU5m5CRLLz0R5AhvABA/hGm0K+ck44dSx980q07ejT98YNBadSo2FRVFfs5ebI0cmTqMOITZMJhwgsAoCvCFDJqbc2+Rqj7usOHM9cKVVZ2DUMzZpyYT1yfbF1FBeEGAJBfhKkhIBqNNXv1pkYocV1TU/rjl5R0DTxjxkizZvUMPsmC0ciRsaYqAAAGK/6MDQLOxQJNb/sNJfYfSjcmjlnPsHPqqdmFoVGjpPLy/rpzAAAKH2FqgLS1de1I3dtg1NaW/vjDh3cNOVOnSnPnZheGKit5fRwAgL4iTGUpGpUaG/vWb6i+PtYJO51QKBZwEsPOzJnZhaFRo2IdowEAwMAbUmGqubnvTWUNDZm/bTVyZNeQ09FvKFkY6h6MysvpSA0AwGBUtGHqrbekpUu7hqKWlvT7lJd3DTsTJ8b6DmUThior+QwEAABDUdGGqbKyWMCZOjXz6/VVVbFapdLSvF4yAAAYhIo2TM2YIf37v+f7KgAAQLHjHS4AAAAPhCkAAAAPhCkAAAAPhCkAAAAPhCkAAAAPhCkAAAAPhCkAAAAPhCkAAAAPhCkAAAAPhCkAAAAPhCkAAAAPhCkAAAAPhCkAAAAP5pzLz4nNaiXtHYBTjZFUNwDnKUTc+9A1lO9/KN+7NLTvn3sfugbi/qc758Ym25C3MDVQzGybc64m39eRD9z70Lx3aWjf/1C+d2lo3z/3PjTvXcr//dPMBwAA4IEwBQAA4GEohKl1+b6APOLeh66hfP9D+d6loX3/3PvQldf7L/o+UwAAAP1pKNRMAQAA9JuiCFNmdr+ZHTSzN1JsNzP7vpn92cxeN7OzBvoa+0sW977QzBrMbGd8un2gr7G/mNlUM/uNmb1pZr83s5uTlCnmZ5/N/Rfl8zezMjP7DzN7LX7v/y1JmaJ89lnee1E+9w5mFjSzV83sySTbivK5J8pw/8X+7N8xs13xe9uWZHtenn9oIE4yANZLukfShhTbPyNpVnw6V9La+M9isF7p712SfuucWzwwlzOg2iX9vXNuh5lVStpuZpudc39IKFPMzz6b+5eK8/m3SLrAOddoZmFJL5jZr5xzLyWUKdZnn829S8X53DvcLOlNSSOSbCvW554o3f1Lxf3sJelTzrlUY0rl5fkXRc2Uc26rpENpilwqaYOLeUnSKDObODBX17+yuPei5Zw74JzbEZ8/qtg/LpO7FSvmZ5/N/Rel+PNsjC+G41P3DqBF+eyzvPeiZWZTJP2/kn6UokhRPvcOWdz/UJeX518UYSoLkyXtS1jeryHyRydufrxJ4Fdmdlq+L6Y/mNkMSWdKernbpiHx7NPcv1Skzz/e1LFT0kFJm51zQ+bZZ3HvUpE+d0l3S/pHSdEU24v2ucfdrfT3LxXvs5di/+Pw72a23cyWJ9mel+c/VMKUJVk3VP5PbodiQ+CfIel/SfpFfi8n98ysQtLjklY4545035xkl6J69hnuv2ifv3Mu4pybJ2mKpHPMbE63IkX77LO496J87ma2WNJB59z2dMWSrCuK557l/Rfls0/wV865sxRrzrvRzBZ0256X5z9UwtR+SVMTlqdIej9P1zKgnHNHOpoEnHNPSwqb2Zg8X1bOxPuMPC7pIefcE0mKFPWzz3T/xf78Jck5d1jSc5Iu6rapqJ+9lPrei/i5/5WkS8zsHUkPS7rAzB7sVqaYn3vG+y/iZy9Jcs69H/95UNJGSed0K5KX5z9UwtQvJX053sv/PEkNzrkD+b6ogWBmE8zM4vPnKPbMP8rvVeVG/L5+LOlN59xdKYoV7bPP5v6L9fmb2VgzGxWfL5f0XyS91a1YUT77bO69WJ+7c+6fnHNTnHMzJC2R9Kxz7kvdihXlc5eyu/9iffaSZGbD4y/byMyGS1okqfub7Hl5/kXxNp+Z/VzSQkljzGy/pFWKdcqUc+4+SU9L+qykP0s6Lmlpfq4097K498slXW9m7ZKaJC1xxTNS619JukbSrnj/EUm6TdI0qfifvbK7/2J9/hMl/cTMgor9sXjUOfekmV0nFf2zz+bei/W5JzVEnntKQ+jZj5e0MZ4VQ5J+5pzbVAjPnxHQAQAAPAyVZj4AAIB+QZgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCUJDMLGKxL8N3TP81h8eeYWbdx6cBgD4pinGmABSlpvgnUwCgoFEzBWBQMbN3zOx/mNl/xKf/FF8/3cx+bWavx39Oi68fb2Yb4x9+fc3MPhk/VNDMfmhmvzezf4+PJg4AvUaYAlCoyrs1812VsO2Ic+4cSfdIuju+7h5JG5xzp0t6SNL34+u/L+n5+Idfz5L0+/j6WZLudc6dJumwpM/3690AKFqMgA6gIJlZo3OuIsn6dyRd4JzbE//Q8wfOudFmVidponOuLb7+gHNujJnVSprinGtJOMYMSZudc7Piy7dKCjvn1gzArQEoMtRMARiMXIr5VGWSaUmYj4g+pAD6iDAFYDC6KuHni/H5/yNpSXz+akkvxOd/Lel6STKzoJmNGKiLBDA08H9iAApVuZntTFje5JzrGB6h1MxeVux/CL8QX/d1Sfeb2S2SanXia/E3S1pnZn+rWA3U9ZIO9PfFAxg66DMFYFCJ95mqcc7V5ftaAECimQ8AAMALNVMAAAAeqJkCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADwQJgCAADw8H8BtSkUTxOpWwIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = {'loss':'r', 'accuracy':'b', 'val_loss':'m', 'val_accuracy':'g'}\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Training Curve\") \n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "for measure in hist.keys():\n",
    "    color = colors[measure]\n",
    "    plt.plot(range(1,epochs+1), hist[measure], color + '-', label=measure)  # use last 2 values to draw line\n",
    "\n",
    "plt.legend(loc='upper left', scatterpoints = 1, frameon=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Augmentation\n",
    "\n",
    "Increase the training set by adding more images: Rotate, shift, flip and scale the original images to generate additional examples that will help the Neural Network to generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(random_state) # enforce repeatable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.2947 - accuracy: 0.1196 - val_loss: 2.2566 - val_accuracy: 0.1567\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 2.2545 - accuracy: 0.1496 - val_loss: 2.1981 - val_accuracy: 0.1824\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 2.2184 - accuracy: 0.1683 - val_loss: 2.1477 - val_accuracy: 0.2065\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.1957 - accuracy: 0.1819 - val_loss: 2.1153 - val_accuracy: 0.2252\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.1751 - accuracy: 0.1913 - val_loss: 2.1076 - val_accuracy: 0.2152\n"
     ]
    }
   ],
   "source": [
    "# recreate and recompile the model (otherwise we continue learning)\n",
    "model = createMyModel()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "history = model.fit(datagen.flow(train_img, image_classes, batch_size=32),\n",
    "                    #steps_per_epoch=len(train_img),\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set (with Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "Accuracy on test set with augmentation: 0.2152\n"
     ]
    }
   ],
   "source": [
    "# verify accuracy on test set with augmentation\n",
    "predictions = model.predict(test_img)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "accuracy_on_test_set = accuracy_score(test_image_labels_for_comparison, predicted_classes)\n",
    "print('Accuracy on test set with augmentation: ' + str(accuracy_on_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Plotting the Training Curve with Data Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23ca314d490>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = history.history\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Training Curve\") \n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "for measure in hist.keys():\n",
    "    color = colors[measure]\n",
    "    plt.plot(range(1,epochs+1), hist[measure], color + '-', label=measure)  # use last 2 values to draw line\n",
    "\n",
    "plt.legend(loc='upper left', scatterpoints = 1, frameon=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is not ideal, but we see both training (blue) and validation (green) Accuracy going up, the training loss and validation loss going continuously down."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
